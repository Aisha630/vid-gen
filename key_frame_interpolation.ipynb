{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for interpolation of Keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add pipeline for keyframe generation here as well (IMP)\n",
    "# DONE : Stitch the final video together after running interpolation on keyframe pairs\n",
    "# DONE : Figure out why the kernel starts shaking crying throwing up when I run the interpolation code. (IMP)\n",
    "# Basically it dies at the last iteration for some reason. If i move the pipe out to save the time for loading it for every interpolation, it dies at second iteration. My thinking is that it is related to the cache/pipe/scehular being deleted/emtpied at wrong time/or maybe just that memeory becomes full but honeslty the latter dont be making sense cuz of the variation in wehn it does as described above\n",
    "# TODO : Compare across different models (LATER)\n",
    "# TODO : Add a function for displaying graphs (LATER)\n",
    "# TODO : Compare across settings. (LATER)\n",
    "# TODO : Add a function for calculating the PSNR and SSIM or whatever similarity metric we decide to use (IMP)\n",
    "# TODO : Add the requirements.txt for this notebook (LATER)\n",
    "# TODO : Make the markdown cells more informative, add description of what each cell does (LATER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > our_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This code uses the `diffusers-0-27-0` environment\n",
    "<br>\n",
    "<span style=\"color:red\">**NOTE:** Define all imports and install/shell commands here</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# add install or other terminal commands here\n",
    "# %pip install numpy matplotlib opencv-python scikit-image scikit-video pillow\n",
    "# %pip install tabulate\n",
    "# %conda install -c conda-forge ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "import sys\n",
    "from KeyFrameDetector.key_frame_detector import keyframeDetectionByChunks\n",
    "from KeyFrameDetector.key_frame_detector import smartKeyframeDetection\n",
    "from utils.extract_frames import strawman_frame_extraction\n",
    "from utils.katna import num_frames_keyframe\n",
    "from utils.perceptual_similarity import calculate_lpips_distance\n",
    "from utils.extract_frames import process_and_extract_frames\n",
    "import numpy as np\n",
    "from moviepy.editor import concatenate_videoclips, VideoFileClip\n",
    "from tabulate import tabulate\n",
    "from svd.attn_ctrl.attention_control import (\n",
    "    AttentionStore,\n",
    "    register_temporal_self_attention_control,\n",
    "    register_temporal_self_attention_flip_control,\n",
    ")\n",
    "from svd.custom_diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler\n",
    "from svd.custom_diffusers.pipelines.pipeline_frame_interpolation_with_noise_injection import FrameInterpolationWithNoiseInjectionPipeline\n",
    "from diffusers import UNetSpatioTemporalConditionModel\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "from utils.ffmpeg_evaluator import FFmpegEvaluator\n",
    "\n",
    "os.chdir(\"/root/VideoReconstruction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "There are two functuions in this pipeline:\n",
    "\n",
    "1. `interpolate_keyframes` - This function takes in 2 keyframes and interpolates the keyframes to get the intermediate keyframes.\n",
    "2. `generate_video` - This function uses a list of keyframes and interpolates between every two consecutive keyframes to get a list of intermediate video segments which are then concatenated to get the final video.\n",
    "\n",
    "```python\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interpolation(checkpoint_dir, frame1_path, frame2_path, out_path, resize_specs, fps, pretrained_model_name_or_path, duration, num_frames, seed=42, num_inference_steps=50, weighted_average=False, noise_injection_steps=0, noise_injection_ratio=0.5, decode_chunk_size=8, device=\"cuda:0\"):\n",
    "    \"\"\"\n",
    "    Run key frame interpolation between two frames using a pretrained model and noise injection.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_dir (str): Directory containing the checkpoint for the fine-tuned UNet model.\n",
    "        frame1_path (str): Path to the first frame image.\n",
    "        frame2_path (str): Path to the second frame image.\n",
    "        out_path (str): Path to save the output interpolated video or gif.\n",
    "        resize_specs (tuple): Tuple specifying the resize dimensions (width, height) for the input frames.\n",
    "        fps (int): Frames per second for the output video.\n",
    "        pretrained_model_name_or_path (str): Path or name of the pretrained model.\n",
    "        duration (int): Duration for which each frame will be displayed in the gif (in milliseconds).\n",
    "        num_frames (int): Number of frames to interpolate between the two input frames.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        num_inference_steps (int, optional): Number of inference steps for the interpolation. Defaults to 50.\n",
    "        weighted_average (bool, optional): Whether to use weighted average during interpolation. Defaults to False.\n",
    "        noise_injection_steps (int, optional): Number of steps for noise injection. Defaults to 0.\n",
    "        noise_injection_ratio (float, optional): Ratio of noise injection. Defaults to 0.5.\n",
    "        decode_chunk_size (int, optional): Chunk size for decoding. Defaults to 8.\n",
    "        device (str, optional): Device to run the interpolation on. Defaults to \"cuda:0\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load noise scheduler and pipeline\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "    pipe = FrameInterpolationWithNoiseInjectionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path, scheduler=noise_scheduler, variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Set up UNet model for fine-tuning and load state dicts\n",
    "\n",
    "    ref_unet = pipe.ori_unet\n",
    "    state_dict = pipe.unet.state_dict()\n",
    "\n",
    "    # Compute delta weights\n",
    "    finetuned_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        checkpoint_dir, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "\n",
    "    ori_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"unet\", variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Apply delta to state dict for specific layers\n",
    "    finetuned_state_dict = finetuned_unet.state_dict()\n",
    "    ori_state_dict = ori_unet.state_dict()\n",
    "    for name, param in finetuned_state_dict.items():\n",
    "        if \"temporal_transformer_blocks.0.attn1.to_v\" in name or \"temporal_transformer_blocks.0.attn1.to_out.0\" in name:\n",
    "            delta_w = param - ori_state_dict[name]\n",
    "            state_dict[name] = state_dict[name] + delta_w\n",
    "    pipe.unet.load_state_dict(state_dict)\n",
    "\n",
    "    # Setup attention controllers\n",
    "    controller_ref = AttentionStore()\n",
    "    register_temporal_self_attention_control(ref_unet, controller_ref)\n",
    "\n",
    "    controller = AttentionStore()\n",
    "    register_temporal_self_attention_flip_control(\n",
    "        pipe.unet, controller, controller_ref)\n",
    "\n",
    "    # Move pipeline to specified device\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "    # Set random seed\n",
    "    generator = torch.Generator(device=device)\n",
    "    if seed is not None:\n",
    "        generator = generator.manual_seed(seed)\n",
    "\n",
    "    # Load and resize frames\n",
    "    frame1 = load_image(frame1_path).resize(resize_specs)\n",
    "    frame2 = load_image(frame2_path).resize(resize_specs)\n",
    "\n",
    "    # Perform inference\n",
    "    print(f\"frame 1 path \", frame1_path)\n",
    "    print(f\"frame 2 path \", frame2_path)\n",
    "\n",
    "    timestamp_f1 = frame1_path.split(\"_\")[1]\n",
    "    timestamp_f2 = frame2_path.split(\"_\")[1]\n",
    "    print(\n",
    "        f\"after splitting the list for f1 is {timestamp_f1} and for f2 is {timestamp_f2}\")\n",
    "    timestamp_f1 = timestamp_f1.split(\".jpg\")[0]\n",
    "    timestamp_f2 = timestamp_f2.split(\".jpg\")[0]\n",
    "    print(f\"timestamp_f1 : {timestamp_f1}\")\n",
    "    print(f\"timestamp_f2 : {timestamp_f2}\")\n",
    "\n",
    "    num_frames = int(\n",
    "        np.round((float(timestamp_f2) - float(timestamp_f1)) * fps))\n",
    "\n",
    "    if num_frames > 24:\n",
    "        num_frames = 24\n",
    "    elif num_frames < 2:\n",
    "        num_frames = 2\n",
    "\n",
    "    print(f\"num_frames : {num_frames}\")\n",
    "\n",
    "    frames = pipe(image1=frame1, image2=frame2, num_inference_steps=num_inference_steps, generator=generator, weighted_average=weighted_average,\n",
    "                  noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, num_frames=num_frames).frames[0]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    # duration = the time  for whicch each frame will be displayed in the gif\n",
    "    if out_path.endswith(\".gif\"):\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as gif\")\n",
    "        frames[0].save(out_path, save_all=True,\n",
    "                       append_images=frames[1:], duration=duration, loop=0)\n",
    "    else:\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as video\")\n",
    "        export_to_video(frames, out_path, fps=fps)\n",
    "\n",
    "    print(f\"Interpolated video saved to {out_path}\")\n",
    "\n",
    "    # Free GPU memory after inference\n",
    "    del controller, controller_ref, ori_unet, finetuned_unet\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 40,
>>>>>>> 4be08a0 (script for smart kfd, round instead of ceil)
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_interpolations(checkpoint_dir, input_sub_dir, output_dir, model_name, num_frames, decode_chunk_size, extension=\".gif\", resize_specs=(1024, 576), fps=7, duration=142, seed=42, inference_steps=20, noise_injection_steps=2, noise_injection_ratio=0.5, device=\"cuda:0\", video_name=None):\n",
    "    \"\"\"\n",
    "    Generates interpolated videos from key frames in the input directory.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_dir (str): Directory containing the model checkpoints.\n",
    "        input_sub_dir (str): Directory containing the input frames.\n",
    "        output_dir (str): Directory to save the output videos.\n",
    "        model_name (str): Name of the pre-trained model to use for interpolation.\n",
    "        num_frames (int): Number of frames to generate between each pair of key frames.\n",
    "        decode_chunk_size (int): Size of the chunk to decode.\n",
    "        extension (str, optional): Extension of the output video files. Defaults to \".gif\".\n",
    "        resize_specs (tuple, optional): Tuple specifying the width and height to resize the frames. Defaults to (1024, 576).\n",
    "        fps (int, optional): Frames per second for the output video. Defaults to 7.\n",
    "        duration (int, optional): Duration of the output video in seconds. Defaults to 142.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        inference_steps (int, optional): Number of inference steps for the model. Defaults to 20.\n",
    "        noise_injection_steps (int, optional): Number of steps to inject noise during inference. Defaults to 2.\n",
    "        noise_injection_ratio (float, optional): Ratio of noise to inject during inference. Defaults to 0.5.\n",
    "        device (str, optional): Device to run the model on. Defaults to \"cuda:0\".\n",
    "        video_name (str, optional): Name of the video. If None, it will be derived from the input_sub_dir. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # video_name = os.path.basename(input_sub_dir)\n",
    "\n",
    "    # print(f\"Generating video for {video_name}\")\n",
    "    video_name = os.path.basename(input_sub_dir)\n",
    "\n",
    "    intermediate_videos_dir = os.path.join(\n",
    "        output_dir, f\"interm_videos_{video_name}\")\n",
    "    os.makedirs(intermediate_videos_dir, exist_ok=True)\n",
    "\n",
    "    frames = sorted([os.path.join(input_sub_dir, f) for f in os.listdir(\n",
    "        input_sub_dir) if f.endswith((\".png\", \".jpeg\", \".jpg\"))])\n",
    "\n",
    "    print(f\"Found {len(frames)} frames in {input_sub_dir}\")\n",
    "    if len(frames) < 2:\n",
    "        print(f\"Skipping {video_name} as it has less than 2 frames\")\n",
    "        return\n",
    "\n",
    "    print(f\"Frames : {frames}\")\n",
    "    frames = sorted(frames, key=lambda x: float(\n",
    "        x.split(\"_\")[-1].replace(\".jpg\", \"\")))\n",
    "    print(f\"Frames : {frames}\")\n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1_path = frames[i]\n",
    "        frame2_path = frames[i + 1]\n",
    "\n",
    "        print(f\"Interpolating between frames {frame1_path} and {frame2_path}\")\n",
    "\n",
    "        segment_output_path = os.path.join(\n",
    "            intermediate_videos_dir, f\"segment_{i}{extension}\")\n",
    "\n",
    "        if os.path.exists(segment_output_path):\n",
    "            print(f\"Segment {i} already completely saved\")\n",
    "            continue\n",
    "\n",
    "        run_interpolation(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            frame1_path=frame1_path,\n",
    "            frame2_path=frame2_path,\n",
    "            out_path=segment_output_path,\n",
    "            resize_specs=resize_specs,\n",
    "            fps=fps,\n",
    "            pretrained_model_name_or_path=model_name,\n",
    "            duration=duration,\n",
    "            seed=seed,\n",
    "            num_inference_steps=inference_steps,\n",
    "            noise_injection_ratio=noise_injection_ratio,\n",
    "            noise_injection_steps=noise_injection_steps,\n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_video_segments(intermediate_videos_dir, output_path, fps):\n",
    "    segment_paths = sorted([os.path.join(intermediate_videos_dir, f)\n",
    "                           for f in os.listdir(intermediate_videos_dir)])\n",
    "\n",
    "    print(f\"Found {len(segment_paths)} segments to stitch.\")\n",
    "    clips = [VideoFileClip(segment).set_fps(fps) for segment in segment_paths]\n",
    "    final_video = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_video.write_videofile(output_path, fps=fps)\n",
    "    print(f\"Final video saved to {output_path}\")\n",
    "\n",
    "    for clip in clips:\n",
    "        clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyframe Generation and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code takes all the videos in input folder and generates keyframes for each video and stores them in the output folder. It also benchmarks the sizes of the original videos and videos from keyframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For /root/VideoReconstruction/input_resized/people_barn.mp4, selected 2 frames, and inserted 11 frames.\n",
      "For /root/VideoReconstruction/input_resized/man_desert.mp4, selected 2 frames, and inserted 9 frames.\n",
      "For /root/VideoReconstruction/input_resized/horse_grass.mp4, selected 2 frames, and inserted 11 frames.\n",
      "For /root/VideoReconstruction/input_resized/woman_meadow.mp4, selected 2 frames, and inserted 9 frames.\n",
      "For /root/VideoReconstruction/input_resized/clouds.mp4, selected 2 frames, and inserted 10 frames.\n",
      "For /root/VideoReconstruction/input_resized/car_drive.mp4, selected 2 frames, and inserted 11 frames.\n",
      "For /root/VideoReconstruction/input_resized/horses_water.mp4, selected 2 frames, and inserted 8 frames.\n",
      "For /root/VideoReconstruction/input_resized/office_man.mp4, selected 2 frames, and inserted 10 frames.\n",
      "For /root/VideoReconstruction/input_resized/ocean.mp4, selected 2 frames, and inserted 9 frames.\n",
      "For /root/VideoReconstruction/input_resized/lamb.mp4, selected 2 frames, and inserted 9 frames.\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| Video Name       | Original Size   |   Total Frames | FPS       |   Total Keyframes | Keyframes Size   | Compressed Video Size   | Savings Original Video   | Savings Keyframes Video   |\n",
      "+==================+=================+================+===========+===================+==================+=========================+==========================+===========================+\n",
      "| people_barn.mp4  | 2.99 MB         |            301 | 30.00 FPS |                13 | 2.39 MB          | 0.05 MB                 | 98.28 %                  | 78.39 %                   |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| man_desert.mp4   | 1.48 MB         |            250 | 25.00 FPS |                11 | 1.52 MB          | 0.03 MB                 | 97.77 %                  | 100.33 %                  |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| horse_grass.mp4  | 0.75 MB         |            300 | 29.97 FPS |                13 | 3.03 MB          | 0.05 MB                 | 93.68 %                  | 397.53 %                  |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| woman_meadow.mp4 | 1.58 MB         |            240 | 24.00 FPS |                11 | 1.44 MB          | 0.03 MB                 | 98.38 %                  | 89.21 %                   |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| clouds.mp4       | 0.78 MB         |            300 | 29.97 FPS |                12 | 0.89 MB          | 0.01 MB                 | 98.61 %                  | 112.61 %                  |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| car_drive.mp4    | 1.76 MB         |            300 | 29.97 FPS |                13 | 2.11 MB          | 0.03 MB                 | 98.39 %                  | 118.08 %                  |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| horses_water.mp4 | 3.15 MB         |            241 | 24.00 FPS |                10 | 2.04 MB          | 0.05 MB                 | 98.56 %                  | 63.49 %                   |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| office_man.mp4   | 0.82 MB         |            251 | 25.00 FPS |                12 | 1.17 MB          | 0.01 MB                 | 98.29 %                  | 141.38 %                  |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| ocean.mp4        | 2.41 MB         |            250 | 25.00 FPS |                11 | 2.29 MB          | 0.04 MB                 | 98.22 %                  | 93.38 %                   |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n",
      "| lamb.mp4         | 0.94 MB         |            241 | 23.98 FPS |                11 | 1.69 MB          | 0.03 MB                 | 97.27 %                  | 176.62 %                  |\n",
      "+------------------+-----------------+----------------+-----------+-------------------+------------------+-------------------------+--------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dir = \"/root/VideoReconstruction/input_resized\"\n",
    "output_dir_base = \"/root/VideoReconstruction/outputs/bucketed_keyframes\"\n",
    "smaller_video_output_dir = \"/root/VideoReconstruction/outputs/bucketed_keyframes_video\"\n",
    "\n",
    "os.makedirs(output_dir_base, exist_ok=True)\n",
    "os.makedirs(smaller_video_output_dir, exist_ok=True)\n",
    "\n",
    "videos = os.listdir(input_dir)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "evaluator = FFmpegEvaluator()\n",
    "\n",
    "for video in videos:\n",
    "    video_path = os.path.join(input_dir, video)\n",
    "    video_name = os.path.splitext(video)[0]\n",
    "    output_dir = os.path.join(output_dir_base, video_name)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "\n",
    "    video_size_in_bytes = os.path.getsize(video_path)\n",
    "    video_size_in_megabytes = video_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    keyframes = smartKeyframeDetection(video_path, output_dir, threshold = 0.3, bucket_size_in_frames = 80, minimum_frames_between=20, maximum_frames_between = 30)\n",
    "    # keyframes = keyframeDetectionByChunks(video_path, output_dir, number_frames_per_bucket=60, top_k=5, verbose=False, minimum_frames_between=20, maximum_frames_between=30)\n",
    "    keyframe_count = len(keyframes)\n",
    "\n",
    "    keyframes_dir_size_in_bytes = get_directory_size(output_dir)\n",
    "    keyframes_dir_size_in_megabytes = keyframes_dir_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    output_dir = f\"{output_dir}/keyFrames\"\n",
    "    compressed_size, savings_from_original_video, savings_from_keyframes_video = evaluator.evaluate_smaller_video(\n",
    "        video_path, smaller_video_output_dir, output_dir)\n",
    "\n",
    "    summary_data.append(\n",
    "        [\n",
    "            video,\n",
    "            f\"{video_size_in_megabytes:.2f} MB\",\n",
    "            total_frames,\n",
    "            f\"{fps:.2f} FPS\",\n",
    "            keyframe_count,\n",
    "            f\"{keyframes_dir_size_in_megabytes:.2f} MB\",\n",
    "            f\"{compressed_size/(1024*1024):.2f} MB\",\n",
    "            f\"{savings_from_original_video:.2f} %\",\n",
    "            f\"{savings_from_keyframes_video:.2f} %\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "headers = [\n",
    "    \"Video Name\",\n",
    "    \"Original Size\",\n",
    "    \"Total Frames\",\n",
    "    \"FPS\",\n",
    "    \"Total Keyframes\",\n",
    "    \"Keyframes Size\",\n",
    "    \"Compressed Video Size\",\n",
    "    \"Savings Original Video\",\n",
    "    \"Savings Keyframes Video\"\n",
    "]\n",
    "print(tabulate(summary_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Settings for Image-to-Video Interpolation:\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Parameter                    | Value                                         |\n",
      "+==============================+===============================================+\n",
      "| Model                        | stabilityai/stable-video-diffusion-img2vid-xt |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Output Directory             | /root/VideoReconstruction/our_results         |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Input Directory              | lambshery                                     |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Resize Specifications        | 1024 x 576                                    |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| FPS                          | 29                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Frame Duration (ms)          | 142                                           |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Inference Steps              | 20                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Random Seed                  | 42                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Saving interpolated video as | mp4 video file                                |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Number of Frames             | 7                                             |\n",
      "+------------------------------+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery\"\n",
    "# output_dir = \"/VideoReconstruction/interim\"\n",
    "output_dir = \"/root/VideoReconstruction/interim/keyFrames\"\n",
    "threshold = 0.3\n",
    "\n",
    "\n",
    "# Model options and selection. maybe we cam add more models later for this\n",
    "models_to_try = [\"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "                 \"stabilityai/stable-video-diffusion-img2vid-xt-1-1\"]\n",
    "MODEL_NAME = models_to_try[0]\n",
    "\n",
    "# Noise injection parameters\n",
    "noise_injection_steps = 2\n",
    "noise_injection_ratio = 0.5\n",
    "\n",
    "# All Directory paths are defined here\n",
    "# CHECKPOINT_DIR = \"/home/iml2/interpolation/svd_keyframe_interpolation/checkpoints/svd_reverse_motion_with_attnflip/svd_reverse_motion_with_attnflip/unet\" #path for iml2\n",
    "CHECKPOINT_DIR = \"/root/VideoReconstruction/svd/checkpoints/svd_reverse_motion_with_attnflip/svd_reverse_motion_with_attnflip/unet\"  # path for docker image\n",
    "OUT_DIR = \"/root/VideoReconstruction/our_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# if this is .gif, the output will be a gif otherwise it will be a video\n",
    "interpolation_extension = \".mp4\"\n",
    "final_extension = \".mp4\"\n",
    "\n",
    "# Resize specifications, frame rate, and duration etc\n",
    "resize_specs = (1024, 576)\n",
    "fps = 29\n",
    "duration = 142  # the duration for which each frame is displayed in the vid\n",
    "inference_steps = 20\n",
    "seed = 42\n",
    "decode_chunk_size = 6\n",
    "num_frames = 7\n",
    "\n",
    "\n",
    "# INPUT_DIR = \"/home/iml2/manframes\"\n",
    "INPUT_DIR = \"/root/VideoReconstruction/outputs/bucketedkeyframes\"\n",
    "input_subdir_name = \"lambshery\"\n",
    "# INPUT_DIR = \"/VideoReconstruction/input\"\n",
    "# input_subdir_name = \"horses\"\n",
    "input_subdir_path = os.path.join(INPUT_DIR, input_subdir_name)\n",
    "\n",
    "# # the katana measure to get frames but they come without timestamps\n",
    "# num_frames_keyframe(video_path, output_dir, num_frames)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Display Configuration\n",
    "# ===========================\n",
    "\n",
    "settings = [\n",
    "    [\"Model\", MODEL_NAME],\n",
    "    [\"Output Directory\", OUT_DIR],\n",
    "    [\"Input Directory\", input_subdir_name],\n",
    "    [\"Resize Specifications\", f\"{resize_specs[0]} x {resize_specs[1]}\"],\n",
    "    [\"FPS\", fps],\n",
    "    [\"Frame Duration (ms)\", duration],\n",
    "    [\"Inference Steps\", inference_steps],\n",
    "    [\"Random Seed\", seed],\n",
    "    [\"Saving interpolated video as\", \"gif file\" if interpolation_extension ==\n",
    "        \".gif\" else \"mp4 video file\"],\n",
    "    [\"Number of Frames\", num_frames],\n",
    "]\n",
    "\n",
    "print(\"\\nSettings for Image-to-Video Interpolation:\")\n",
    "print(tabulate(settings, headers=[\"Parameter\", \"Value\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] No such file or directory: '/root/VideoReconstruction/our_results/interm_videos_lamb'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 34\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m \u001b[43mstart_time\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop over each file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Check if the file is an mp4 file\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        start_time = time.time()\n",
    "        # Get the base name without the extension - WHAT IS HAPPENING HERE\n",
    "        video_name = os.path.splitext(filename)[0]\n",
    "        # Define the full path to the video file\n",
    "        video_path = os.path.join(input_dir, filename)\n",
    "        # keyframeDetection(video_path, output_dir, threshold)\n",
    "        strawman_frame_extraction(video_path, output_dir)\n",
    "\n",
    "        try:\n",
    "            generate_all_interpolations(checkpoint_dir=CHECKPOINT_DIR, input_sub_dir=input_subdir_path, output_dir=OUT_DIR, model_name=MODEL_NAME, resize_specs=resize_specs, fps=fps, duration=duration, extension=interpolation_extension,\n",
    "                                        seed=seed, inference_steps=inference_steps, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, decode_chunk_size=decode_chunk_size, num_frames=num_frames, video_name=video_name)\n",
    "        except Exception as e:\n",
    "            print(\"Error while generate_all_interpolations\")\n",
    "            print(f\"Error: {e}\")\n",
    "            # i am doing this cuz it is annoying to get out of memory erros every time there is an error. so we will kill the process after it is done/error thrown\n",
    "            print(\"Killing the kernel processes\")\n",
    "            env_name = os.path.basename(sys.prefix)\n",
    "            subprocess.call(\n",
    "                f\"nvidia-smi | grep {env_name} | awk '{{print $5}}' | xargs -I {{}} kill -9 {{}}\",\n",
    "                shell=True\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    try:\n",
    "        stitch_video_segments(intermediate_videos_dir=os.path.join(OUT_DIR, f\"interm_videos_{video_name}\"), output_path=os.path.join(\n",
    "            OUT_DIR, f\"{video_name}_interpolated{final_extension}\"), fps=fps)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time for {video_name}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 frames in /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery\n",
      "Frames : ['/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame0_0.0000.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame101_4.2118.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame132_5.5046.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame161_6.7139.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame189_7.8816.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame213_8.8824.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame239_9.9666.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame36_1.5013.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame69_2.8774.jpg']\n",
      "Frames : ['/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame0_0.0000.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame36_1.5013.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame69_2.8774.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame101_4.2118.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame132_5.5046.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame161_6.7139.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame189_7.8816.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame213_8.8824.jpg', '/root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame239_9.9666.jpg']\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame0_0.0000.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame36_1.5013.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2481a88a4540b59d6fab6d337bf07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame0_0.0000.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame36_1.5013.jpg\n",
      "after splitting the list for f1 is 0.0000.jpg and for f2 is 1.5013.jpg\n",
      "timestamp_f1 : 0.0000\n",
      "timestamp_f2 : 1.5013\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e15afbc9710439389a0c013b561aa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_0.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_0.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame36_1.5013.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame69_2.8774.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e999589dca4f2992d9ef4156531b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame36_1.5013.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame69_2.8774.jpg\n",
      "after splitting the list for f1 is 1.5013.jpg and for f2 is 2.8774.jpg\n",
      "timestamp_f1 : 1.5013\n",
      "timestamp_f2 : 2.8774\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a690eb27d7b43499a38bdd44e132d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_1.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_1.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame69_2.8774.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame101_4.2118.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e21c0e91585416393bc37ccab5c900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame69_2.8774.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame101_4.2118.jpg\n",
      "after splitting the list for f1 is 2.8774.jpg and for f2 is 4.2118.jpg\n",
      "timestamp_f1 : 2.8774\n",
      "timestamp_f2 : 4.2118\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff82e4818e8f47f6be909ec4a5330a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_2.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_2.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame101_4.2118.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame132_5.5046.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9b1309f0b44c708a7a9f1bfe43b033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame101_4.2118.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame132_5.5046.jpg\n",
      "after splitting the list for f1 is 4.2118.jpg and for f2 is 5.5046.jpg\n",
      "timestamp_f1 : 4.2118\n",
      "timestamp_f2 : 5.5046\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19547682a6af4e9db909e40bb6df7369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_3.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_3.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame132_5.5046.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame161_6.7139.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b57ee5b558433a9503c60b791360c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame132_5.5046.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame161_6.7139.jpg\n",
      "after splitting the list for f1 is 5.5046.jpg and for f2 is 6.7139.jpg\n",
      "timestamp_f1 : 5.5046\n",
      "timestamp_f2 : 6.7139\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcea72b8fba44a39ee202dfb18831f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_4.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_4.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame161_6.7139.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame189_7.8816.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06620ccf44a9426a8602cdf68fbded26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame161_6.7139.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame189_7.8816.jpg\n",
      "after splitting the list for f1 is 6.7139.jpg and for f2 is 7.8816.jpg\n",
      "timestamp_f1 : 6.7139\n",
      "timestamp_f2 : 7.8816\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed18d2f064c4daf97bdbb3fd68494c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_5.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_5.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame189_7.8816.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame213_8.8824.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143822d7c96a4881ae3871454ae6ccba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame189_7.8816.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame213_8.8824.jpg\n",
      "after splitting the list for f1 is 7.8816.jpg and for f2 is 8.8824.jpg\n",
      "timestamp_f1 : 7.8816\n",
      "timestamp_f2 : 8.8824\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3adde5de58b4a478413b9f99f5c476d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_6.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_6.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame213_8.8824.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame239_9.9666.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3c1292d4aa42968e15946be66105d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame213_8.8824.jpg\n",
      "frame 2 path  /root/VideoReconstruction/outputs/bucketedkeyframes/lambshery/frame239_9.9666.jpg\n",
      "after splitting the list for f1 is 8.8824.jpg and for f2 is 9.9666.jpg\n",
      "timestamp_f1 : 8.8824\n",
      "timestamp_f2 : 9.9666\n",
      "num_frames : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d97cc35eaf44238b04c086a07c25ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 24 frames to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_7.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_lambshery/segment_7.mp4\n",
      "Found 8 segments to stitch.\n",
      "Moviepy - Building video /root/VideoReconstruction/our_results/lambshery_interpolated.mp4.\n",
      "Moviepy - Writing video /root/VideoReconstruction/our_results/lambshery_interpolated.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/VideoReconstruction/our_results/lambshery_interpolated.mp4\n",
      "Final video saved to /root/VideoReconstruction/our_results/lambshery_interpolated.mp4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    generate_all_interpolations(checkpoint_dir=CHECKPOINT_DIR, input_sub_dir=input_subdir_path, output_dir=OUT_DIR, model_name=MODEL_NAME, resize_specs=resize_specs, fps=fps, duration=duration, extension=interpolation_extension,\n",
    "                    seed=seed, inference_steps=inference_steps, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, decode_chunk_size=decode_chunk_size, num_frames=num_frames)\n",
    "except Exception as e:\n",
    "    print(\"Error while generate_all_interpolations\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # i am doing this cuz it is annoying to get out of memory erros every time there is an error. so we will kill the process after it is done/error thrown\n",
    "    print(\"Killing the kernel processes\")\n",
    "    env_name = os.path.basename(sys.prefix)\n",
    "    subprocess.call(\n",
    "    f\"nvidia-smi | grep {env_name} | awk '{{print $5}}' | xargs -I {{}} kill -9 {{}}\",\n",
    "    shell=True\n",
    "    )\n",
    "\n",
    "try:\n",
    "    stitch_video_segments(intermediate_videos_dir=os.path.join(OUT_DIR, f\"interm_videos_{os.path.basename(input_dir)}\"), output_path=os.path.join(\n",
    "        OUT_DIR, f\"{input_subdir_name}_interpolated{final_extension}\"), fps=fps)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Elapsed time for {video_name}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi | grep $(conda env list | grep '*' | awk '{print $1}') | awk '{print $5}' | xargs -I {} kill -9 {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate the generated video against the original video using several methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted frames saved in 'frames/input'.\n",
      "Extracted frames saved in 'frames/output'.\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Average LPIPs loss: 0.22561180187123162\n"
     ]
    }
   ],
   "source": [
    "# Adjust the paths and other hyper parameters of the two videos to compare against each other.\n",
    "generated_video = \"office_dude/1.mp4\"\n",
    "# generated_video = \"our_results_strawman/car_drive_interpolated.mp4\"\n",
    "original_video = \"office_dude/2.mp4\"\n",
    "# original_video = \"input/car_drive.mp4\"\n",
    "frame_width = 1024\n",
    "frame_height = 576\n",
    "max_frames = 300\n",
    "\n",
    "# make directories for storing the frames of both of these videos if they dont exist already\n",
    "frames_dir = \"frames\"\n",
    "if os.path.exists(frames_dir):\n",
    "    shutil.rmtree(frames_dir)\n",
    "generated_video_frame_dir = os.path.join(frames_dir, \"input\")\n",
    "original_video_frame_dir = os.path.join(frames_dir, \"output\")\n",
    "if not os.path.exists(frames_dir):\n",
    "    os.makedirs(frames_dir)\n",
    "if not os.path.exists(generated_video_frame_dir):\n",
    "    os.makedirs(generated_video_frame_dir)\n",
    "if not os.path.exists(original_video_frame_dir):\n",
    "    os.makedirs(original_video_frame_dir)\n",
    "\n",
    "original_video_dir = \"/VideoReconstruction/input\"\n",
    "generated_video_dir = \"/VideoReconstruction/our_results_strawman\"\n",
    "\n",
    "# # get file basename form original video dir\n",
    "# for file in os.listdir(original_video_dir):\n",
    "#     if file.endswith(\".mp4\"):\n",
    "#         filename = os.path.splitext(file)[0]\n",
    "#         print(f\"Processing {filename}\")\n",
    "#         generated_video = f\"{generated_video_dir}/{filename}_interpolated.mp4\"\n",
    "#         original_video = f\"{original_video_dir}/{filename}.mp4\"\n",
    "#         # break\n",
    "#         # break both the videos down into frames\n",
    "process_and_extract_frames(input_video_path=generated_video, frames_output_dir=generated_video_frame_dir,\n",
    "                           new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "\n",
    "process_and_extract_frames(input_video_path=original_video, frames_output_dir=original_video_frame_dir,\n",
    "                           new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "# Pass it the directories with the frames of original and generated video to retrieve the perceptual loss across each frame\n",
    "calculate_lpips_distance(dir0=generated_video_frame_dir,\n",
    "                         dir1=original_video_frame_dir, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPIPs perceptual similarity method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "frame_0020.png: 0.626\n",
      "frame_0017.png: 0.633\n",
      "frame_0011.png: 0.623\n",
      "frame_0003.png: 0.586\n",
      "frame_0006.png: 0.576\n",
      "frame_0001.png: 0.551\n",
      "frame_0012.png: 0.619\n",
      "frame_0018.png: 0.638\n",
      "frame_0007.png: 0.576\n",
      "frame_0009.png: 0.613\n",
      "frame_0010.png: 0.623\n",
      "frame_0000.png: 0.506\n",
      "frame_0016.png: 0.620\n",
      "frame_0008.png: 0.598\n",
      "frame_0014.png: 0.607\n",
      "frame_0004.png: 0.578\n",
      "frame_0005.png: 0.579\n",
      "frame_0002.png: 0.574\n",
      "frame_0015.png: 0.592\n",
      "frame_0019.png: 0.632\n",
      "frame_0013.png: 0.614\n",
      "frame_0025.png: 0.618\n",
      "frame_0043.png: 0.563\n",
      "frame_0029.png: 0.628\n",
      "frame_0055.png: 0.557\n",
      "frame_0052.png: 0.599\n",
      "frame_0042.png: 0.562\n",
      "frame_0050.png: 0.598\n",
      "frame_0037.png: 0.604\n",
      "frame_0030.png: 0.616\n",
      "frame_0048.png: 0.589\n",
      "frame_0051.png: 0.598\n",
      "frame_0064.png: 0.578\n",
      "frame_0024.png: 0.606\n",
      "frame_0031.png: 0.598\n",
      "frame_0069.png: 0.605\n",
      "frame_0063.png: 0.570\n",
      "frame_0044.png: 0.560\n",
      "frame_0038.png: 0.599\n",
      "frame_0023.png: 0.596\n",
      "frame_0058.png: 0.601\n",
      "frame_0045.png: 0.558\n",
      "frame_0028.png: 0.623\n",
      "frame_0027.png: 0.622\n",
      "frame_0021.png: 0.626\n",
      "frame_0060.png: 0.614\n",
      "frame_0061.png: 0.605\n",
      "frame_0056.png: 0.578\n",
      "frame_0068.png: 0.611\n",
      "frame_0035.png: 0.621\n",
      "frame_0053.png: 0.592\n",
      "frame_0041.png: 0.565\n",
      "frame_0046.png: 0.557\n",
      "frame_0059.png: 0.608\n",
      "frame_0032.png: 0.613\n",
      "frame_0040.png: 0.573\n",
      "frame_0047.png: 0.557\n",
      "frame_0033.png: 0.622\n",
      "frame_0071.png: 0.566\n",
      "frame_0070.png: 0.596\n",
      "frame_0036.png: 0.615\n",
      "frame_0039.png: 0.582\n",
      "frame_0049.png: 0.600\n",
      "frame_0026.png: 0.621\n",
      "frame_0034.png: 0.620\n",
      "frame_0054.png: 0.575\n",
      "frame_0066.png: 0.603\n",
      "frame_0065.png: 0.599\n",
      "frame_0057.png: 0.594\n",
      "frame_0022.png: 0.614\n",
      "frame_0062.png: 0.585\n",
      "frame_0067.png: 0.608\n",
      "Average LPIPs loss: 0.5963292585478889\n"
     ]
    }
   ],
   "source": [
    "# Pass it the directories with the frames of original and generated video to retrieve the perceptual loss across each frame\n",
    "calculate_lpips_distance(dir0=generated_video_frame_dir,\n",
    "                         dir1=original_video_frame_dir, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViSiL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrainFart",
     "evalue": "I am hongies and I am on a break",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrainFart\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, message):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(message)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m BrainFart(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am hongies and I am on a break\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mBrainFart\u001b[0m: I am hongies and I am on a break"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean-up of the Frames directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(frames_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-0-27-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
