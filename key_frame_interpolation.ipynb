{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for interpolation of Keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add pipeline for keyframe generation here as well (IMP)\n",
    "# DONE : Stitch the final video together after running interpolation on keyframe pairs\n",
    "# DONE : Figure out why the kernel starts shaking crying throwing up when I run the interpolation code. (IMP)\n",
    "# Basically it dies at the last iteration for some reason. If i move the pipe out to save the time for loading it for every interpolation, it dies at second iteration. My thinking is that it is related to the cache/pipe/scehular being deleted/emtpied at wrong time/or maybe just that memeory becomes full but honeslty the latter dont be making sense cuz of the variation in wehn it does as described above\n",
    "# TODO : Compare across different models (LATER)\n",
    "# TODO : Add a function for displaying graphs (LATER)\n",
    "# TODO : Compare across settings. (LATER)\n",
    "# TODO : Add a function for calculating the PSNR and SSIM or whatever similarity metric we decide to use (IMP)\n",
    "# TODO : Add the requirements.txt for this notebook (LATER)\n",
    "# TODO : Make the markdown cells more informative, add description of what each cell does (LATER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > our_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This code uses the `diffusers-0-27-0` environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "## add install or other terminal commands here\n",
    "# %pip install numpy matplotlib opencv-python scikit-image scikit-video pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from KeyFrameDetector.key_frame_detector import keyframeDetectionByChunks\n",
    "from utils.extract_frames import strawman_frame_extraction\n",
    "from utils.katna import num_frames_keyframe\n",
    "from utils.perceptual_similarity import calculate_lpips_distance\n",
    "from utils.extract_frames import process_and_extract_frames\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL\n",
    "import numpy as np\n",
    "from moviepy.editor import concatenate_videoclips, VideoFileClip\n",
    "from tabulate import tabulate\n",
    "from attn_ctrl.attention_control import (\n",
    "    AttentionStore,\n",
    "    register_temporal_self_attention_control,\n",
    "    register_temporal_self_attention_flip_control,\n",
    ")\n",
    "from custom_diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler\n",
    "from custom_diffusers.pipelines.pipeline_frame_interpolation_with_noise_injection import FrameInterpolationWithNoiseInjectionPipeline\n",
    "from diffusers import UNetSpatioTemporalConditionModel\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "os.chdir(\"/VideoReconstruction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "There are two functuions in this pipeline:\n",
    "\n",
    "1. `interpolate_keyframes` - This function takes in 2 keyframes and interpolates the keyframes to get the intermediate keyframes.\n",
    "2. `generate_video` - This function uses a list of keyframes and interpolates between every two consecutive keyframes to get a list of intermediate video segments which are then concatenated to get the final video.\n",
    "\n",
    "```python\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interpolation(checkpoint_dir, frame1_path, frame2_path, out_path, resize_specs, fps, pretrained_model_name_or_path, duration, num_frames, seed=42, num_inference_steps=50, weighted_average=False, noise_injection_steps=0, noise_injection_ratio=0.5, decode_chunk_size=8, device=\"cuda:0\"):\n",
    "    # Load noise scheduler and pipeline\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "    pipe = FrameInterpolationWithNoiseInjectionPipeline.from_pretrained(pretrained_model_name_or_path, scheduler=noise_scheduler, variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Set up UNet model for fine-tuning and load state dicts\n",
    "\n",
    "    ref_unet = pipe.ori_unet\n",
    "    state_dict = pipe.unet.state_dict()\n",
    "\n",
    "    # Compute delta weights\n",
    "    finetuned_unet = UNetSpatioTemporalConditionModel.from_pretrained(checkpoint_dir, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "\n",
    "    ori_unet = UNetSpatioTemporalConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\", variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Apply delta to state dict for specific layers\n",
    "    finetuned_state_dict = finetuned_unet.state_dict()\n",
    "    ori_state_dict = ori_unet.state_dict()\n",
    "    for name, param in finetuned_state_dict.items():\n",
    "        if \"temporal_transformer_blocks.0.attn1.to_v\" in name or \"temporal_transformer_blocks.0.attn1.to_out.0\" in name:\n",
    "            delta_w = param - ori_state_dict[name]\n",
    "            state_dict[name] = state_dict[name] + delta_w\n",
    "    pipe.unet.load_state_dict(state_dict)\n",
    "\n",
    "    # Setup attention controllers\n",
    "    controller_ref = AttentionStore()\n",
    "    register_temporal_self_attention_control(ref_unet, controller_ref)\n",
    "\n",
    "    controller = AttentionStore()\n",
    "    register_temporal_self_attention_flip_control(pipe.unet, controller, controller_ref)\n",
    "\n",
    "    # Move pipeline to specified device\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "    # Set random seed\n",
    "    generator = torch.Generator(device=device)\n",
    "    if seed is not None:\n",
    "        generator = generator.manual_seed(seed)\n",
    "\n",
    "    # Load and resize frames\n",
    "    frame1 = load_image(frame1_path).resize(resize_specs)\n",
    "    frame2 = load_image(frame2_path).resize(resize_specs)\n",
    "\n",
    "    # Perform inference\n",
    "    print(f\"frame 1 path \", frame1_path)\n",
    "    print(f\"frame 2 path \", frame2_path)\n",
    "\n",
    "    timestamp_f1 = frame1_path.split(\"_\")[1]\n",
    "    timestamp_f2 = frame2_path.split(\"_\")[1]\n",
    "    print(f\"after splitting the list for f1 is {timestamp_f1} and for f2 is {timestamp_f2}\")\n",
    "    timestamp_f1 = timestamp_f1.split(\".jpg\")[0]\n",
    "    timestamp_f2 = timestamp_f2.split(\".jpg\")[0]\n",
    "    print(f\"timestamp_f1 : {timestamp_f1}\")\n",
    "    print(f\"timestamp_f2 : {timestamp_f2}\")\n",
    "\n",
    "    num_frames = int(np.ceil((float(timestamp_f2) - float(timestamp_f1)) * fps))\n",
    "\n",
    "    if num_frames > 24:\n",
    "        num_frames = 24\n",
    "    elif num_frames < 2:\n",
    "        num_frames = 2\n",
    "\n",
    "    print(f\"num_frames : {num_frames}\")\n",
    "\n",
    "    frames = pipe(image1=frame1, image2=frame2, num_inference_steps=num_inference_steps, generator=generator, weighted_average=weighted_average, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, num_frames=num_frames).frames[0]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    # duration = the time  for whicch each frame will be displayed in the gif\n",
    "    if out_path.endswith(\".gif\"):\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as gif\")\n",
    "        frames[0].save(out_path, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "    else:\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as video\")\n",
    "        export_to_video(frames, out_path, fps=fps)\n",
    "\n",
    "    print(f\"Interpolated video saved to {out_path}\")\n",
    "\n",
    "    # Free GPU memory after inference\n",
    "    del controller, controller_ref, ori_unet, finetuned_unet\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_interpolations(checkpoint_dir, input_sub_dir, output_dir, model_name, num_frames, decode_chunk_size, extension=\".gif\", resize_specs=(1024, 576), fps=7, duration=142, seed=42, inference_steps=20, noise_injection_steps=2, noise_injection_ratio=0.5, device=\"cuda:0\", video_name=None):\n",
    "    # video_name = os.path.basename(input_sub_dir)\n",
    "\n",
    "    print(f\"Generating video for {video_name}\")\n",
    "\n",
    "    intermediate_videos_dir = os.path.join(output_dir, f\"interm_videos_{video_name}\")\n",
    "    os.makedirs(intermediate_videos_dir, exist_ok=True)\n",
    "\n",
    "    frames = sorted([os.path.join(input_sub_dir, f) for f in os.listdir(input_sub_dir) if f.endswith((\".png\", \".jpeg\", \".jpg\"))])\n",
    "\n",
    "    print(f\"Found {len(frames)} frames in {input_sub_dir}\")\n",
    "    if len(frames) < 2:\n",
    "        print(f\"Skipping {video_name} as it has less than 2 frames\")\n",
    "        return\n",
    "\n",
    "    print(f\"Frames : {frames}\")\n",
    "    frames = sorted(frames, key=lambda x: float(x.split(\"_\")[-1].replace(\".jpg\", \"\")))\n",
    "    print(f\"Frames : {frames}\")\n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1_path = frames[i]\n",
    "        frame2_path = frames[i + 1]\n",
    "\n",
    "        print(f\"Interpolating between frames {frame1_path} and {frame2_path}\")\n",
    "\n",
    "        segment_output_path = os.path.join(intermediate_videos_dir, f\"segment_{i}{extension}\")\n",
    "\n",
    "        run_interpolation(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            frame1_path=frame1_path,\n",
    "            frame2_path=frame2_path,\n",
    "            out_path=segment_output_path,\n",
    "            resize_specs=resize_specs,\n",
    "            fps=fps,\n",
    "            pretrained_model_name_or_path=model_name,\n",
    "            duration=duration,\n",
    "            seed=seed,\n",
    "            num_inference_steps=inference_steps,\n",
    "            noise_injection_ratio=noise_injection_ratio,\n",
    "            noise_injection_steps=noise_injection_steps,\n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_video_segments(intermediate_videos_dir, output_path, fps):\n",
    "    segment_paths = sorted([os.path.join(intermediate_videos_dir, f) for f in os.listdir(intermediate_videos_dir)])\n",
    "\n",
    "    print(f\"Found {len(segment_paths)} segments to stitch.\")\n",
    "    clips = [VideoFileClip(segment).set_fps(fps) for segment in segment_paths]\n",
    "    final_video = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_video.write_videofile(output_path, fps=fps)\n",
    "    print(f\"Final video saved to {output_path}\")\n",
    "\n",
    "    for clip in clips:\n",
    "        clip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keyframes(input_dir, output_dir):\n",
    "    # print(f\"Generating keyframes for {input_dir}\")\n",
    "    keyframes = keyframeDetectionByChunks(input_dir, output_dir, x=40, k=2, verbose=False)\n",
    "    # print(f\"Keyframes saved to {output_dir}\")\n",
    "    return keyframes\n",
    "\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Key frames using bucketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video length (frames): 301\n",
      "FPS: 30.0\n",
      "Video length (frames): 250\n",
      "FPS: 25.0\n",
      "Video length (frames): 300\n",
      "FPS: 29.97\n",
      "Video length (frames): 240\n",
      "FPS: 24.0\n",
      "Video length (frames): 300\n",
      "FPS: 29.97\n",
      "Video length (frames): 300\n",
      "FPS: 29.97\n",
      "Video length (frames): 241\n",
      "FPS: 24.0\n",
      "Video length (frames): 251\n",
      "FPS: 25.0\n",
      "Video length (frames): 250\n",
      "FPS: 25.0\n",
      "Video length (frames): 241\n",
      "FPS: 23.98\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| Video Name       | Video Size   |   Total Frames | FPS       |   Total Keyframes | Keyframes Size   |\n",
      "+==================+==============+================+===========+===================+==================+\n",
      "| people_barn.mp4  | 2.99 MB      |            301 | 30.00 FPS |                32 | 5.86 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| man_desert.mp4   | 1.48 MB      |            250 | 25.00 FPS |                27 | 3.67 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| horse_grass.mp4  | 0.75 MB      |            300 | 29.97 FPS |                32 | 7.45 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| woman_meadow.mp4 | 1.58 MB      |            240 | 24.00 FPS |                24 | 3.18 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| clouds.mp4       | 0.78 MB      |            300 | 29.97 FPS |                31 | 2.34 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| car_drive.mp4    | 1.76 MB      |            300 | 29.97 FPS |                31 | 5.05 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| horses_water.mp4 | 3.15 MB      |            241 | 24.00 FPS |                25 | 5.14 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| office_man.mp4   | 0.82 MB      |            251 | 25.00 FPS |                28 | 2.78 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| ocean.mp4        | 2.41 MB      |            250 | 25.00 FPS |                27 | 5.68 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n",
      "| lamb.mp4         | 0.94 MB      |            241 | 23.98 FPS |                25 | 3.86 MB          |\n",
      "+------------------+--------------+----------------+-----------+-------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/root/VideoReconstruction/input_resized\"\n",
    "output_dir_base = \"/root/VideoReconstruction/bucketed_keyframes\"\n",
    "\n",
    "videos = os.listdir(input_dir)\n",
    "summary_data = []\n",
    "\n",
    "for video in videos:\n",
    "    video_path = os.path.join(input_dir, video)\n",
    "    output_dir = os.path.join(output_dir_base, video.split(\".\")[0])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "\n",
    "    video_size_in_bytes = os.path.getsize(video_path)\n",
    "    video_size_in_megabytes = video_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    keyframes = generate_keyframes(video_path, output_dir)\n",
    "    keyframe_count = len(keyframes)\n",
    "\n",
    "    keyframes_dir = output_dir\n",
    "    keyframes_dir_size_in_bytes = get_directory_size(keyframes_dir)\n",
    "    keyframes_dir_size_in_megabytes = keyframes_dir_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    summary_data.append(\n",
    "        [\n",
    "            video,\n",
    "            f\"{video_size_in_megabytes:.2f} MB\",\n",
    "            total_frames,\n",
    "            f\"{fps:.2f} FPS\",\n",
    "            keyframe_count,\n",
    "            f\"{keyframes_dir_size_in_megabytes:.2f} MB\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "headers = [\"Video Name\", \"Video Size\", \"Total Frames\", \"FPS\", \"Total Keyframes\", \"Keyframes Size\"]\n",
    "print(tabulate(summary_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing people_barn.mp4\n",
      "Video duration: 10.0 seconds, FPS: 30\n",
      "Finished extracting frames\n",
      "Total size of keyframes for people_barn.mp4: 8.28 MB\n",
      "Processing man_desert.mp4\n",
      "Video duration: 10.0 seconds, FPS: 25\n",
      "Finished extracting frames\n",
      "Total size of keyframes for man_desert.mp4: 6.78 MB\n",
      "Processing horse_grass.mp4\n",
      "Video duration: 10.344827586206897 seconds, FPS: 29\n",
      "Finished extracting frames\n",
      "Total size of keyframes for horse_grass.mp4: 14.40 MB\n",
      "Processing woman_meadow.mp4\n",
      "Video duration: 10.0 seconds, FPS: 24\n",
      "Finished extracting frames\n",
      "Total size of keyframes for woman_meadow.mp4: 6.21 MB\n",
      "Processing clouds.mp4\n",
      "Video duration: 10.344827586206897 seconds, FPS: 29\n",
      "Finished extracting frames\n",
      "Total size of keyframes for clouds.mp4: 3.20 MB\n",
      "Processing car_drive.mp4\n",
      "Video duration: 10.344827586206897 seconds, FPS: 29\n",
      "Finished extracting frames\n",
      "Total size of keyframes for car_drive.mp4: 9.02 MB\n",
      "Processing horses_water.mp4\n",
      "Video duration: 10.0 seconds, FPS: 24\n",
      "Finished extracting frames\n",
      "Total size of keyframes for horses_water.mp4: 8.25 MB\n",
      "Processing office_man.mp4\n",
      "Video duration: 10.0 seconds, FPS: 25\n",
      "Finished extracting frames\n",
      "Total size of keyframes for office_man.mp4: 3.91 MB\n",
      "Processing ocean.mp4\n",
      "Video duration: 10.0 seconds, FPS: 25\n",
      "Finished extracting frames\n",
      "Total size of keyframes for ocean.mp4: 9.80 MB\n",
      "Processing lamb.mp4\n",
      "Video duration: 10.434782608695652 seconds, FPS: 23\n",
      "Finished extracting frames\n",
      "Total size of keyframes for lamb.mp4: 7.87 MB\n"
     ]
    }
   ],
   "source": [
    "# # temporarily commented out\n",
    "video_name = \"people_barn\"\n",
    "video_path = f\"/VideoReconstruction/input/{video_name}.mp4\"\n",
    "output_dir = \"/VideoReconstruction/interim/keyFrames\"\n",
    "threshold = 0.5\n",
    "\n",
    "\n",
    "# def get_directory_size(directory):\n",
    "#     total_size = 0\n",
    "#     for dirpath, dirnames, filenames in os.walk(directory):\n",
    "#         for filename in filenames:\n",
    "#             # print(f\"filename : {filename}\")\n",
    "#             filepath = os.path.join(dirpath, filename)\n",
    "#             total_size += os.path.getsize(filepath)\n",
    "#     return total_size\n",
    "\n",
    "# input_dir = \"/VideoReconstruction/input\"\n",
    "# output_dir = \"/VideoReconstruction/interim/\"\n",
    "# threshold = 0.5  # Set your desired threshold here\n",
    "\n",
    "# for video in os.listdir(input_dir):\n",
    "#     video_path = os.path.join(input_dir, video)\n",
    "#     print(f\"Processing {video}\")\n",
    "\n",
    "#     # Run keyframe detection\n",
    "#     # keyframeDetection(video_path, output_dir, threshold)\n",
    "#     strawman_frame_extraction(video_path, output_dir)\n",
    "\n",
    "\n",
    "#     # # Define the keyframes directory path\n",
    "#     # keyframes_dir = os.path.join(output_dir, \"keyFrames\")\n",
    "#     keyframes_dir = output_dir\n",
    "\n",
    "#     # # Calculate and print the size of the keyframes directory\n",
    "#     size_in_bytes = get_directory_size(keyframes_dir)\n",
    "#     # total_size = os.path.getsize(video_path)\n",
    "#     size_in_megabytes = size_in_bytes / (1024 * 1024)\n",
    "#     print(f\"Total size of keyframes for {video}: {size_in_megabytes:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Settings for Image-to-Video Interpolation:\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Parameter                    | Value                                         |\n",
      "+==============================+===============================================+\n",
      "| Model                        | stabilityai/stable-video-diffusion-img2vid-xt |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Output Directory             | /VideoReconstruction/our_results              |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Input Directory              | keyFrames                                     |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Resize Specifications        | 1024 x 576                                    |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| FPS                          | 7                                             |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Frame Duration (ms)          | 142                                           |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Inference Steps              | 10                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Random Seed                  | 42                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Saving interpolated video as | mp4 video file                                |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Number of Frames             | 7                                             |\n",
      "+------------------------------+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/VideoReconstruction/input\"\n",
    "# output_dir = \"/VideoReconstruction/interim\"\n",
    "output_dir = \"/VideoReconstruction/interim/keyFrames\"\n",
    "threshold = 0.3\n",
    "\n",
    "\n",
    "# Model options and selection. maybe we cam add more models later for this\n",
    "models_to_try = [\"stabilityai/stable-video-diffusion-img2vid-xt\", \"stabilityai/stable-video-diffusion-img2vid-xt-1-1\"]\n",
    "MODEL_NAME = models_to_try[0]\n",
    "\n",
    "# Noise injection parameters\n",
    "noise_injection_steps = 2\n",
    "noise_injection_ratio = 0.5\n",
    "\n",
    "# All Directory paths are defined here\n",
    "# CHECKPOINT_DIR = \"/home/iml2/interpolation/svd_keyframe_interpolation/checkpoints/svd_reverse_motion_with_attnflip/svd_reverse_motion_with_attnflip/unet\" #path for iml2\n",
    "CHECKPOINT_DIR = \"/VideoReconstruction/svd/checkpoints/svd_reverse_motion_with_attnflip/svd_reverse_motion_with_attnflip/unet\"  # path for docker image\n",
    "OUT_DIR = \"/VideoReconstruction/our_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# if this is .gif, the output will be a gif otherwise it will be a video\n",
    "interpolation_extension = \".mp4\"\n",
    "final_extension = \".mp4\"\n",
    "\n",
    "# Resize specifications, frame rate, and duration etc\n",
    "resize_specs = (1024, 576)\n",
    "fps = 7\n",
    "duration = 142  # the duration for which each frame is displayed in the vid\n",
    "inference_steps = 10\n",
    "seed = 42\n",
    "decode_chunk_size = 6\n",
    "num_frames = 7\n",
    "\n",
    "\n",
    "# INPUT_DIR = \"/home/iml2/manframes\"\n",
    "INPUT_DIR = \"/VideoReconstruction/interim/\"\n",
    "input_subdir_name = \"keyFrames\"\n",
    "# INPUT_DIR = \"/VideoReconstruction/input\"\n",
    "# input_subdir_name = \"horses\"\n",
    "input_subdir_path = os.path.join(INPUT_DIR, input_subdir_name)\n",
    "\n",
    "# # the katana measure to get frames but they come without timestamps\n",
    "# num_frames_keyframe(video_path, output_dir, num_frames)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Display Configuration\n",
    "# ===========================\n",
    "\n",
    "settings = [\n",
    "    [\"Model\", MODEL_NAME],\n",
    "    [\"Output Directory\", OUT_DIR],\n",
    "    [\"Input Directory\", input_subdir_name],\n",
    "    [\"Resize Specifications\", f\"{resize_specs[0]} x {resize_specs[1]}\"],\n",
    "    [\"FPS\", fps],\n",
    "    [\"Frame Duration (ms)\", duration],\n",
    "    [\"Inference Steps\", inference_steps],\n",
    "    [\"Random Seed\", seed],\n",
    "    [\"Saving interpolated video as\", \"gif file\" if interpolation_extension == \".gif\" else \"mp4 video file\"],\n",
    "    [\"Number of Frames\", num_frames],\n",
    "]\n",
    "\n",
    "print(\"\\nSettings for Image-to-Video Interpolation:\")\n",
    "print(tabulate(settings, headers=[\"Parameter\", \"Value\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video duration: 10.0 seconds, FPS: 30\n",
      "Finished extracting frames\n",
      "Generating video for people_barn\n",
      "Found 5 frames in /VideoReconstruction/interim/keyFrames\n",
      "Frames : ['/VideoReconstruction/interim/keyFrames/1_0.00.jpg', '/VideoReconstruction/interim/keyFrames/2_2.00.jpg', '/VideoReconstruction/interim/keyFrames/3_4.00.jpg', '/VideoReconstruction/interim/keyFrames/4_6.00.jpg', '/VideoReconstruction/interim/keyFrames/5_8.00.jpg']\n",
      "Frames : ['/VideoReconstruction/interim/keyFrames/1_0.00.jpg', '/VideoReconstruction/interim/keyFrames/2_2.00.jpg', '/VideoReconstruction/interim/keyFrames/3_4.00.jpg', '/VideoReconstruction/interim/keyFrames/4_6.00.jpg', '/VideoReconstruction/interim/keyFrames/5_8.00.jpg']\n",
      "Interpolating between frames /VideoReconstruction/interim/keyFrames/1_0.00.jpg and /VideoReconstruction/interim/keyFrames/2_2.00.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aef8f81e86a41ff8277b94c9333390b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /VideoReconstruction/interim/keyFrames/1_0.00.jpg\n",
      "frame 2 path  /VideoReconstruction/interim/keyFrames/2_2.00.jpg\n",
      "after splitting the list for f1 is 0.00.jpg and for f2 is 2.00.jpg\n",
      "timestamp_f1 : 0.00\n",
      "timestamp_f2 : 2.00\n",
      "num_frames : 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79883892b9a4e7aa5de221eb7f5c9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 14 frames to /VideoReconstruction/our_results/interm_videos_people_barn/segment_0.mp4 as video\n",
      "Interpolated video saved to /VideoReconstruction/our_results/interm_videos_people_barn/segment_0.mp4\n",
      "Interpolating between frames /VideoReconstruction/interim/keyFrames/2_2.00.jpg and /VideoReconstruction/interim/keyFrames/3_4.00.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838da800e5ed4bbf9fbc3a7bf79a654b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /VideoReconstruction/interim/keyFrames/2_2.00.jpg\n",
      "frame 2 path  /VideoReconstruction/interim/keyFrames/3_4.00.jpg\n",
      "after splitting the list for f1 is 2.00.jpg and for f2 is 4.00.jpg\n",
      "timestamp_f1 : 2.00\n",
      "timestamp_f2 : 4.00\n",
      "num_frames : 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db6d732d7e44155bc2a3b273336c1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 14 frames to /VideoReconstruction/our_results/interm_videos_people_barn/segment_1.mp4 as video\n",
      "Interpolated video saved to /VideoReconstruction/our_results/interm_videos_people_barn/segment_1.mp4\n",
      "Interpolating between frames /VideoReconstruction/interim/keyFrames/3_4.00.jpg and /VideoReconstruction/interim/keyFrames/4_6.00.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67456e11f17045ec9a9a6c6a571043d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1 path  /VideoReconstruction/interim/keyFrames/3_4.00.jpg\n",
      "frame 2 path  /VideoReconstruction/interim/keyFrames/4_6.00.jpg\n",
      "after splitting the list for f1 is 4.00.jpg and for f2 is 6.00.jpg\n",
      "timestamp_f1 : 4.00\n",
      "timestamp_f2 : 6.00\n",
      "num_frames : 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80a82dfc41a4b39babfdc96aafebbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m strawman_frame_extraction(video_path, output_dir)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mgenerate_all_interpolations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_sub_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_subdir_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresize_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_chunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_name\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m, in \u001b[0;36mgenerate_all_interpolations\u001b[0;34m(checkpoint_dir, input_sub_dir, output_dir, model_name, num_frames, decode_chunk_size, extension, resize_specs, fps, duration, seed, inference_steps, noise_injection_steps, noise_injection_ratio, device, video_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterpolating between frames \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe1_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe2_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     segment_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     46\u001b[0m         intermediate_videos_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mrun_interpolation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe1_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe1_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe2_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe2_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_output_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresize_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     64\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[6], line 100\u001b[0m, in \u001b[0;36mrun_interpolation\u001b[0;34m(checkpoint_dir, frame1_path, frame2_path, out_path, resize_specs, fps, pretrained_model_name_or_path, duration, num_frames, seed, num_inference_steps, weighted_average, noise_injection_steps, noise_injection_ratio, decode_chunk_size, device)\u001b[0m\n\u001b[1;32m     96\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_frames : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweighted_average\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mframes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    112\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(out_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# duration = the time  for whicch each frame will be displayed in the gif\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/VideoReconstruction/custom_diffusers/pipelines/pipeline_frame_interpolation_with_noise_injection.py:543\u001b[0m, in \u001b[0;36mFrameInterpolationWithNoiseInjectionPipeline.__call__\u001b[0;34m(self, image1, image2, height, width, num_frames, num_inference_steps, min_guidance_scale, max_guidance_scale, fps, motion_bucket_id, noise_aug_strength, decode_chunk_size, num_videos_per_prompt, generator, latents, output_type, callback_on_step_end, callback_on_step_end_tensor_inputs, weighted_average, noise_injection_steps, noise_injection_ratio, return_dict)\u001b[0m\n\u001b[1;32m    541\u001b[0m noise \u001b[38;5;241m=\u001b[39m noise \u001b[38;5;241m*\u001b[39m sigma\n\u001b[1;32m    542\u001b[0m latents \u001b[38;5;241m=\u001b[39m latents \u001b[38;5;241m+\u001b[39m noise\n\u001b[0;32m--> 543\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidiffusion_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage1_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage1_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_time_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[1;32m    548\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep(noise_pred, t, latents)\u001b[38;5;241m.\u001b[39mprev_sample\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/VideoReconstruction/custom_diffusers/pipelines/pipeline_frame_interpolation_with_noise_injection.py:294\u001b[0m, in \u001b[0;36mFrameInterpolationWithNoiseInjectionPipeline.multidiffusion_step\u001b[0;34m(self, latents, t, image1_embeddings, image2_embeddings, image1_latents, image2_latents, added_time_ids, avg_weight)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[1;32m    287\u001b[0m noise_pred1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mori_unet(\n\u001b[1;32m    288\u001b[0m     latent_model_input1,\n\u001b[1;32m    289\u001b[0m     t,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    293\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 294\u001b[0m noise_pred2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage2_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_time_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_time_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/diffusers/models/unets/unet_spatio_temporal_condition.py:434\u001b[0m, in \u001b[0;36mUNetSpatioTemporalConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, added_time_ids, return_dict)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m downsample_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(downsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m downsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[0;32m--> 434\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(\n\u001b[1;32m    442\u001b[0m             hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[1;32m    443\u001b[0m             temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[1;32m    444\u001b[0m             image_only_indicator\u001b[38;5;241m=\u001b[39mimage_only_indicator,\n\u001b[1;32m    445\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/diffusers/models/unets/unet_3d_blocks.py:2178\u001b[0m, in \u001b[0;36mCrossAttnDownBlockSpatioTemporal.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, image_only_indicator)\u001b[0m\n\u001b[1;32m   2171\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn(\n\u001b[1;32m   2172\u001b[0m         hidden_states,\n\u001b[1;32m   2173\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   2174\u001b[0m         image_only_indicator\u001b[38;5;241m=\u001b[39mimage_only_indicator,\n\u001b[1;32m   2175\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2176\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2178\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2183\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn(\n\u001b[1;32m   2184\u001b[0m         hidden_states,\n\u001b[1;32m   2185\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   2186\u001b[0m         image_only_indicator\u001b[38;5;241m=\u001b[39mimage_only_indicator,\n\u001b[1;32m   2187\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2188\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2190\u001b[0m output_states \u001b[38;5;241m=\u001b[39m output_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/diffusers/models/resnet.py:698\u001b[0m, in \u001b[0;36mSpatioTemporalResBlock.forward\u001b[0;34m(self, hidden_states, temb, image_only_indicator)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    693\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mFloatTensor,\n\u001b[1;32m    694\u001b[0m     temb: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m     image_only_indicator: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    696\u001b[0m ):\n\u001b[1;32m    697\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m image_only_indicator\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 698\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatial_res_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     batch_frames, channels, height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    701\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_frames \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_frames\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/diffusers/models/resnet.py:376\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_shortcut \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_shortcut(input_tensor)\n\u001b[0;32m--> 376\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scale_factor\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over each file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Check if the file is an mp4 file\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        start_time = time.time()\n",
    "        # Get the base name without the extension\n",
    "        video_name = os.path.splitext(filename)[0]\n",
    "        # Define the full path to the video file\n",
    "        video_path = os.path.join(input_dir, filename)\n",
    "        # keyframeDetection(video_path, output_dir, threshold)\n",
    "        strawman_frame_extraction(video_path, output_dir)\n",
    "\n",
    "        try:\n",
    "            generate_all_interpolations(checkpoint_dir=CHECKPOINT_DIR, input_sub_dir=input_subdir_path, output_dir=OUT_DIR, model_name=MODEL_NAME, resize_specs=resize_specs, fps=fps, duration=duration, extension=interpolation_extension, seed=seed, inference_steps=inference_steps, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, decode_chunk_size=decode_chunk_size, num_frames=num_frames, video_name=video_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            # i am doing this cuz it is annoying to get out of memory erros every time there is an error. so we will kill the process after it is done/error thrown\n",
    "            # print(\"Killing the kernel processes\")\n",
    "            # env_name = os.path.basename(sys.prefix)\n",
    "            # subprocess.call(\n",
    "            #     f\"nvidia-smi | grep {env_name} | awk '{{print $5}}' | xargs -I {{}} kill -9 {{}}\",\n",
    "            #     shell=True\n",
    "            # )\n",
    "            # raise e\n",
    "        # finally:\n",
    "    # End timing for this iteration\n",
    "    try:\n",
    "        stitch_video_segments(intermediate_videos_dir=os.path.join(OUT_DIR, f\"interm_videos_{video_name}\"), output_path=os.path.join(OUT_DIR, f\"{video_name}_interpolated{final_extension}\"), fps=fps)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time for {video_name}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in os.listdir(input_dir):\n",
    "#     # Check if the file is an mp4 file\n",
    "#     if filename.endswith(\".mp4\"):\n",
    "#         video_name = os.path.splitext(filename)[0]\n",
    "#         stitch_video_segments(\n",
    "#             intermediate_videos_dir=os.path.join(\n",
    "#                 OUT_DIR, f\"interm_videos_{video_name}\"),\n",
    "#             output_path=os.path.join(\n",
    "#                 OUT_DIR, f\"{video_name}_interpolated{final_extension}\"),\n",
    "#             fps=fps\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi | grep $(conda env list | grep '*' | awk '{print $1}') | awk '{print $5}' | xargs -I {} kill -9 {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate the generated video against the original video using several methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted frames saved in 'frames/input'.\n",
      "Extracted frames saved in 'frames/output'.\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Average LPIPs loss: 0.22561180187123162\n"
     ]
    }
   ],
   "source": [
    "# Adjust the paths and other hyper parameters of the two videos to compare against each other.\n",
    "generated_video = \"office_dude/1.mp4\"\n",
    "# generated_video = \"our_results_strawman/car_drive_interpolated.mp4\"\n",
    "original_video = \"office_dude/2.mp4\"\n",
    "# original_video = \"input/car_drive.mp4\"\n",
    "frame_width = 1024\n",
    "frame_height = 576\n",
    "max_frames = 300\n",
    "\n",
    "# make directories for storing the frames of both of these videos if they dont exist already\n",
    "frames_dir = \"frames\"\n",
    "if os.path.exists(frames_dir):\n",
    "    shutil.rmtree(frames_dir)\n",
    "generated_video_frame_dir = os.path.join(frames_dir, \"input\")\n",
    "original_video_frame_dir = os.path.join(frames_dir, \"output\")\n",
    "if not os.path.exists(frames_dir):\n",
    "    os.makedirs(frames_dir)\n",
    "if not os.path.exists(generated_video_frame_dir):\n",
    "    os.makedirs(generated_video_frame_dir)\n",
    "if not os.path.exists(original_video_frame_dir):\n",
    "    os.makedirs(original_video_frame_dir)\n",
    "\n",
    "original_video_dir = \"/VideoReconstruction/input\"\n",
    "generated_video_dir = \"/VideoReconstruction/our_results_strawman\"\n",
    "\n",
    "# # get file basename form original video dir\n",
    "# for file in os.listdir(original_video_dir):\n",
    "#     if file.endswith(\".mp4\"):\n",
    "#         filename = os.path.splitext(file)[0]\n",
    "#         print(f\"Processing {filename}\")\n",
    "#         generated_video = f\"{generated_video_dir}/{filename}_interpolated.mp4\"\n",
    "#         original_video = f\"{original_video_dir}/{filename}.mp4\"\n",
    "#         # break\n",
    "#         # break both the videos down into frames\n",
    "process_and_extract_frames(input_video_path=generated_video, frames_output_dir=generated_video_frame_dir, new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "\n",
    "process_and_extract_frames(input_video_path=original_video, frames_output_dir=original_video_frame_dir, new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "# Pass it the directories with the frames of original and generated video to retrieve the perceptual loss across each frame\n",
    "calculate_lpips_distance(dir0=generated_video_frame_dir, dir1=original_video_frame_dir, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPIPs perceptual similarity method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/diffusers-0-27-0/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "frame_0020.png: 0.626\n",
      "frame_0017.png: 0.633\n",
      "frame_0011.png: 0.623\n",
      "frame_0003.png: 0.586\n",
      "frame_0006.png: 0.576\n",
      "frame_0001.png: 0.551\n",
      "frame_0012.png: 0.619\n",
      "frame_0018.png: 0.638\n",
      "frame_0007.png: 0.576\n",
      "frame_0009.png: 0.613\n",
      "frame_0010.png: 0.623\n",
      "frame_0000.png: 0.506\n",
      "frame_0016.png: 0.620\n",
      "frame_0008.png: 0.598\n",
      "frame_0014.png: 0.607\n",
      "frame_0004.png: 0.578\n",
      "frame_0005.png: 0.579\n",
      "frame_0002.png: 0.574\n",
      "frame_0015.png: 0.592\n",
      "frame_0019.png: 0.632\n",
      "frame_0013.png: 0.614\n",
      "frame_0025.png: 0.618\n",
      "frame_0043.png: 0.563\n",
      "frame_0029.png: 0.628\n",
      "frame_0055.png: 0.557\n",
      "frame_0052.png: 0.599\n",
      "frame_0042.png: 0.562\n",
      "frame_0050.png: 0.598\n",
      "frame_0037.png: 0.604\n",
      "frame_0030.png: 0.616\n",
      "frame_0048.png: 0.589\n",
      "frame_0051.png: 0.598\n",
      "frame_0064.png: 0.578\n",
      "frame_0024.png: 0.606\n",
      "frame_0031.png: 0.598\n",
      "frame_0069.png: 0.605\n",
      "frame_0063.png: 0.570\n",
      "frame_0044.png: 0.560\n",
      "frame_0038.png: 0.599\n",
      "frame_0023.png: 0.596\n",
      "frame_0058.png: 0.601\n",
      "frame_0045.png: 0.558\n",
      "frame_0028.png: 0.623\n",
      "frame_0027.png: 0.622\n",
      "frame_0021.png: 0.626\n",
      "frame_0060.png: 0.614\n",
      "frame_0061.png: 0.605\n",
      "frame_0056.png: 0.578\n",
      "frame_0068.png: 0.611\n",
      "frame_0035.png: 0.621\n",
      "frame_0053.png: 0.592\n",
      "frame_0041.png: 0.565\n",
      "frame_0046.png: 0.557\n",
      "frame_0059.png: 0.608\n",
      "frame_0032.png: 0.613\n",
      "frame_0040.png: 0.573\n",
      "frame_0047.png: 0.557\n",
      "frame_0033.png: 0.622\n",
      "frame_0071.png: 0.566\n",
      "frame_0070.png: 0.596\n",
      "frame_0036.png: 0.615\n",
      "frame_0039.png: 0.582\n",
      "frame_0049.png: 0.600\n",
      "frame_0026.png: 0.621\n",
      "frame_0034.png: 0.620\n",
      "frame_0054.png: 0.575\n",
      "frame_0066.png: 0.603\n",
      "frame_0065.png: 0.599\n",
      "frame_0057.png: 0.594\n",
      "frame_0022.png: 0.614\n",
      "frame_0062.png: 0.585\n",
      "frame_0067.png: 0.608\n",
      "Average LPIPs loss: 0.5963292585478889\n"
     ]
    }
   ],
   "source": [
    "# Pass it the directories with the frames of original and generated video to retrieve the perceptual loss across each frame\n",
    "calculate_lpips_distance(dir0=generated_video_frame_dir, dir1=original_video_frame_dir, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViSiL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrainFart",
     "evalue": "I am hongies and I am on a break",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrainFart\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, message):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(message)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m BrainFart(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am hongies and I am on a break\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mBrainFart\u001b[0m: I am hongies and I am on a break"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean-up of the Frames directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(frames_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-0-27-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
