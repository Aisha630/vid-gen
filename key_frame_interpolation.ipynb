{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for interpolation of Keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Compare across different models (LATER)\n",
    "# TODO : Add a function for displaying graphs (LATER)\n",
    "# TODO : Compare across settings. (LATER)\n",
    "# TODO : Add a function for calculating the PSNR and SSIM or whatever similarity metric we decide to use (IMP)\n",
    "# TODO : Add the requirements.txt for this notebook (LATER)\n",
    "# TODO : Make the markdown cells more informative, add description of what each cell does (LATER)\n",
    "# TODO : explore algorithmic interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > our_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This code uses the `diffusers-0-27-0` environment\n",
    "<br>\n",
    "<span style=\"color:red\">**NOTE:** Define all imports and install/shell commands here</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# add install or other terminal commands here\n",
    "# %pip install numpy matplotlib opencv-python scikit-image scikit-video pillow\n",
    "# %pip install tabulate\n",
    "# %conda install -c conda-f/orge ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "import sys\n",
    "from KeyFrameDetector.key_frame_detector import keyframeDetectionByChunks\n",
    "from KeyFrameDetector.key_frame_detector import smartKeyframeDetection\n",
    "from utils.extract_frames import strawman_frame_extraction\n",
    "from utils.katna import num_frames_keyframe\n",
    "from utils.perceptual_similarity import calculate_lpips_distance\n",
    "from utils.extract_frames import process_and_extract_frames\n",
    "import numpy as np\n",
    "from moviepy.editor import concatenate_videoclips, VideoFileClip\n",
    "from tabulate import tabulate\n",
    "from svd.attn_ctrl.attention_control import (\n",
    "    AttentionStore,\n",
    "    register_temporal_self_attention_control,\n",
    "    register_temporal_self_attention_flip_control,\n",
    ")\n",
    "from svd.custom_diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler\n",
    "from svd.custom_diffusers.pipelines.pipeline_frame_interpolation_with_noise_injection import FrameInterpolationWithNoiseInjectionPipeline\n",
    "from diffusers import UNetSpatioTemporalConditionModel\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "from utils.ffmpeg_evaluator import FFmpegEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"/root/VideoReconstruction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "There are two functuions in this pipeline:\n",
    "\n",
    "1. `interpolate_keyframes` - This function takes in 2 keyframes and interpolates the keyframes to get the intermediate keyframes.\n",
    "2. `generate_video` - This function uses a list of keyframes and interpolates between every two consecutive keyframes to get a list of intermediate video segments which are then concatenated to get the final video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "The `run_interpolation` function interpolates between two keyframes and saves the output segment video to the specifed path.\n",
    "\n",
    "The  `generate_all_interpolations` function generates all the intermediate keyframes between every two consecutive keyframes in an input directory with all the keyframes in it saving all the output videos to the output path.\n",
    "\n",
    "The `stitch_video_segments` function stitches the video segments to get the final video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interpolation(checkpoint_dir, frame1_path, frame2_path, out_path, resize_specs, fps, pretrained_model_name_or_path, duration, num_frames, seed=42, num_inference_steps=50, weighted_average=False, noise_injection_steps=0, noise_injection_ratio=0.5, decode_chunk_size=8, device=\"cuda:0\"):\n",
    "    \"\"\"\n",
    "    Run key frame interpolation between two frames using a pretrained model and noise injection. It saves the interpolated video or gif to the specified output path.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_dir (str): Directory containing the checkpoint for the fine-tuned UNet model.\n",
    "        frame1_path (str): Path to the first frame image.\n",
    "        frame2_path (str): Path to the second frame image.\n",
    "        out_path (str): Path to save the output interpolated video or gif.\n",
    "        resize_specs (tuple): Tuple specifying the resize dimensions (width, height) for the input frames.\n",
    "        fps (int): Frames per second for the output video.\n",
    "        pretrained_model_name_or_path (str): Path or name of the pretrained model.\n",
    "        duration (int): Duration for which each frame will be displayed in the gif (in milliseconds).\n",
    "        num_frames (int): Number of frames to interpolate between the two input frames.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        num_inference_steps (int, optional): Number of inference steps for the interpolation. Defaults to 50.\n",
    "        weighted_average (bool, optional): Whether to use weighted average during interpolation. Defaults to False. \n",
    "        True: Produces a video with a gradual shift from image1 to image2. This can give the effect of morphing or smooth interpolation between two images.\n",
    "        False: Maintains an equal influence of both images across all frames. This results in a more consistent combination of features from both image1 and image2.\n",
    "\n",
    "        noise_injection_steps (int, optional): Number of steps for noise injection. Defaults to 0.\n",
    "        noise_injection_ratio (float, optional): Ratio of noise injection. Defaults to 0.5.\n",
    "        decode_chunk_size (int, optional): Chunk size for decoding. Defaults to 8. controls how many frames are decoded at a time from the latent representations during the video generation process.\n",
    "\n",
    "\n",
    "        device (str, optional): Device to run the interpolation on. Defaults to \"cuda:0\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load noise scheduler and pipeline\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "    pipe = FrameInterpolationWithNoiseInjectionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path, scheduler=noise_scheduler, variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Set up UNet model for fine-tuning and load state dicts\n",
    "\n",
    "    ref_unet = pipe.ori_unet\n",
    "    state_dict = pipe.unet.state_dict()\n",
    "\n",
    "    # Compute delta weights\n",
    "    finetuned_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        checkpoint_dir, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "\n",
    "    ori_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"unet\", variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Apply delta to state dict for specific layers\n",
    "    finetuned_state_dict = finetuned_unet.state_dict()\n",
    "    ori_state_dict = ori_unet.state_dict()\n",
    "    for name, param in finetuned_state_dict.items():\n",
    "        if \"temporal_transformer_blocks.0.attn1.to_v\" in name or \"temporal_transformer_blocks.0.attn1.to_out.0\" in name:\n",
    "            delta_w = param - ori_state_dict[name]\n",
    "            state_dict[name] = state_dict[name] + delta_w\n",
    "    pipe.unet.load_state_dict(state_dict)\n",
    "\n",
    "    # Setup attention controllers\n",
    "    controller_ref = AttentionStore()\n",
    "    register_temporal_self_attention_control(ref_unet, controller_ref)\n",
    "\n",
    "    controller = AttentionStore()\n",
    "    register_temporal_self_attention_flip_control(\n",
    "        pipe.unet, controller, controller_ref)\n",
    "\n",
    "    # Move pipeline to specified device\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "    # Set random seed\n",
    "    generator = torch.Generator(device=device)\n",
    "    if seed is not None:\n",
    "        generator = generator.manual_seed(seed)\n",
    "\n",
    "    # Load and resize frames\n",
    "    frame1 = load_image(frame1_path).resize(resize_specs)\n",
    "    frame2 = load_image(frame2_path).resize(resize_specs)\n",
    "\n",
    "    timestamp_f1 = frame1_path.split(\"_\")[1]\n",
    "    timestamp_f2 = frame2_path.split(\"_\")[1]\n",
    "    timestamp_f1 = timestamp_f1.split(\".jpg\")[0]\n",
    "    timestamp_f2 = timestamp_f2.split(\".jpg\")[0]\n",
    "  \n",
    "\n",
    "    num_frames = int(\n",
    "        np.round((float(timestamp_f2) - float(timestamp_f1)) * fps))\n",
    "\n",
    "    if num_frames > 24:\n",
    "        num_frames = 24\n",
    "    elif num_frames < 2:\n",
    "        num_frames = 2\n",
    "\n",
    "\n",
    "    frames = pipe(image1=frame1, image2=frame2, num_inference_steps=num_inference_steps, generator=generator, weighted_average=False,\n",
    "                  noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, num_frames=num_frames, decode_chunk_size=decode_chunk_size).frames[0]\n",
    "    # frames = pipe(image1=frame1, image2=frame2, num_inference_steps=num_inference_steps, generator=generator, weighted_average=True,\n",
    "    #               noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, num_frames=num_frames, decode_chunk_size=7, motion_bucket_id=200).frames[0]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    # duration = the time  for whicch each frame will be displayed in the gif\n",
    "    if out_path.endswith(\".gif\"):\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as gif\")\n",
    "        frames[0].save(out_path, save_all=True,\n",
    "                       append_images=frames[1:], duration=duration, loop=0)\n",
    "    else:\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as video\")\n",
    "        export_to_video(frames, out_path, fps=fps)\n",
    "\n",
    "    print(f\"Interpolated video saved to {out_path}\")\n",
    "\n",
    "    # Free GPU memory after inference\n",
    "    del controller, controller_ref, ori_unet, finetuned_unet\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_interpolations(checkpoint_dir, input_sub_dir, output_dir, model_name, num_frames, decode_chunk_size, extension=\".gif\", resize_specs=(1024, 576), fps=7, duration=142, seed=42, inference_steps=20, noise_injection_steps=2, noise_injection_ratio=0.5, device=\"cuda:0\", video_name=None):\n",
    "    \"\"\"\n",
    "    Generates interpolated videos from key frames in the input directory.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_dir (str): Directory containing the model checkpoints.\n",
    "        input_sub_dir (str): Directory containing the input frames.\n",
    "        output_dir (str): Directory to save the output videos.\n",
    "        model_name (str): Name of the pre-trained model to use for interpolation.\n",
    "        num_frames (int): Number of frames to generate between each pair of key frames.\n",
    "        decode_chunk_size (int): Size of the chunk to decode.\n",
    "        extension (str, optional): Extension of the output video files. Defaults to \".gif\".\n",
    "        resize_specs (tuple, optional): Tuple specifying the width and height to resize the frames. Defaults to (1024, 576).\n",
    "        fps (int, optional): Frames per second for the output video. Defaults to 7.\n",
    "        duration (int, optional): Duration of the output video in seconds. Defaults to 142.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        inference_steps (int, optional): Number of inference steps for the model. Defaults to 20.\n",
    "        noise_injection_steps (int, optional): Number of steps to inject noise during inference. Defaults to 2.\n",
    "        noise_injection_ratio (float, optional): Ratio of noise to inject during inference. Defaults to 0.5.\n",
    "        device (str, optional): Device to run the model on. Defaults to \"cuda:0\".\n",
    "        video_name (str, optional): Name of the video. If None, it will be derived from the input_sub_dir. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # video_name = os.path.basename(input_sub_dir)\n",
    "\n",
    "    # print(f\"Generating video for {video_name}\")\n",
    "    video_name = os.path.basename(input_sub_dir)\n",
    "\n",
    "    intermediate_videos_dir = os.path.join(\n",
    "        output_dir, f\"interm_videos_{video_name}\")\n",
    "    os.makedirs(intermediate_videos_dir, exist_ok=True)\n",
    "\n",
    "    frames = sorted([os.path.join(input_sub_dir, f) for f in os.listdir(\n",
    "        input_sub_dir) if f.endswith((\".png\", \".jpeg\", \".jpg\"))])\n",
    "\n",
    "    print(f\"Found {len(frames)} frames in {input_sub_dir}\")\n",
    "    \n",
    "    if len(frames) < 2:\n",
    "        print(f\"Skipping {video_name} as it has less than 2 frames\")\n",
    "        return\n",
    "\n",
    "    frames = sorted(frames, key=lambda x: float(\n",
    "        x.split(\"_\")[-1].replace(\".jpg\", \"\")))\n",
    "    \n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1_path = frames[i]\n",
    "        frame2_path = frames[i + 1]\n",
    "\n",
    "        print(f\"Interpolating between frames {frame1_path} and {frame2_path}\")\n",
    "\n",
    "        if \"<s>\" in frame1_path:\n",
    "            print(f\"Skipping {frame1_path} as its entrie bucket is already saved\")\n",
    "            continue\n",
    "        \n",
    "        segment_output_path = os.path.join(\n",
    "            intermediate_videos_dir, f\"segment_{i}{extension}\")\n",
    "\n",
    "        run_interpolation(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            frame1_path=frame1_path,\n",
    "            frame2_path=frame2_path,\n",
    "            out_path=segment_output_path,\n",
    "            resize_specs=resize_specs,\n",
    "            fps=fps,\n",
    "            pretrained_model_name_or_path=model_name,\n",
    "            duration=duration,\n",
    "            seed=seed,\n",
    "            num_inference_steps=inference_steps,\n",
    "            noise_injection_ratio=noise_injection_ratio,\n",
    "            noise_injection_steps=noise_injection_steps,\n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_video_segments(intermediate_videos_dir, output_path, fps):\n",
    "    \"\"\"\n",
    "    Stitches together video segments from a specified directory into a single video file.\n",
    "\n",
    "    Args:\n",
    "        intermediate_videos_dir (str): The directory containing the video segments to be stitched together.\n",
    "        output_path (str): The file path where the final stitched video will be saved.\n",
    "        fps (int): The frames per second (fps) rate for the final video.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Prints:\n",
    "        The number of segments found and a confirmation message when the final video is saved.\n",
    "    \"\"\"\n",
    "    segment_paths = sorted([os.path.join(intermediate_videos_dir, f)\n",
    "                           for f in os.listdir(intermediate_videos_dir)])\n",
    "\n",
    "    print(f\"Found {len(segment_paths)} segments to stitch.\")\n",
    "    clips = [VideoFileClip(segment).set_fps(fps) for segment in segment_paths]\n",
    "    final_video = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_video.write_videofile(output_path, fps=fps)\n",
    "    print(f\"Final video saved to {output_path}\")\n",
    "\n",
    "    for clip in clips:\n",
    "        clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyframe Generation and Savings Analysis\n",
    "\n",
    "This part of the code processes all the videos in the input folder, generating keyframes for each video and storing them in the output folder. It also benchmarks the file sizes of the original videos and the videos reconstructed from the keyframes.\n",
    "\n",
    "To ensure a fair comparison, the original videos—initially in 4K resolution—are resized to match the resolution of the videos that can be generated by our system, which is currently limited to 1024x576 due to hardware and model constraints. By resizing the original videos to 1024x576 before conducting the bandwidth savings analysis, we maintain consistency and avoid skewing results caused by differences in resolution.\n",
    "\n",
    "To further standardize the comparison, the same codec (`libx264`) and bitrate are applied to both the resized original videos and the videos reconstructed from keyframes. This ensures that differences in compression settings do not affect the analysis, making the results reflect genuine bandwidth savings attributable to keyframe interpolation.\n",
    "\n",
    "> **Note:** While the output videos are limited to 1024x576 resolution, the keyframes are still generated from the original 4K frames. This approach leverages the higher quality of 4K frames during interpolation, resulting in better-quality reconstructed videos even though the final resolution is reduced to 1024x576. This ensures that the interpolation process operates on the highest possible quality inputs, maximizing the fidelity of the generated videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size_in_frames = 60\n",
    "threshold = 0.3\n",
    "minimum_frames_in_between = 24\n",
    "maximum_frames_in_between = 30\n",
    "segment_fps = 15\n",
    "top_k_no_interpolation = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For /root/VideoReconstruction/input_videos/manwalkingsunglassestrim.mp4, selected 2 frames, and inserted 9 frames.\n",
      "For /root/VideoReconstruction/input_videos/manrunningbeachtrim.mp4, selected 2 frames, and inserted 8 frames.\n",
      "For /root/VideoReconstruction/input_videos/waterfall.mp4, selected 2 frames, and inserted 9 frames.\n",
      "For /root/VideoReconstruction/input_videos/waterwavestrim.mp4, selected 2 frames, and inserted 13 frames.\n",
      "For /root/VideoReconstruction/input_videos/manwalkingcamera1920.mp4, selected 2 frames, and inserted 25 frames.\n",
      "For /root/VideoReconstruction/input_videos/threecyclysistsbut1920.mp4, selected 2 frames, and inserted 13 frames.\n",
      "For /root/VideoReconstruction/input_videos/dogrunning1920.mp4, selected 2 frames, and inserted 46 frames.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7db0f th {\n",
       "  font-weight: bold;\n",
       "  background-color: #000000;\n",
       "}\n",
       "#T_7db0f_row0_col0, #T_7db0f_row0_col1, #T_7db0f_row0_col2, #T_7db0f_row0_col3, #T_7db0f_row0_col4, #T_7db0f_row0_col5, #T_7db0f_row0_col6, #T_7db0f_row0_col7, #T_7db0f_row0_col8, #T_7db0f_row1_col0, #T_7db0f_row1_col1, #T_7db0f_row1_col2, #T_7db0f_row1_col3, #T_7db0f_row1_col4, #T_7db0f_row1_col5, #T_7db0f_row1_col6, #T_7db0f_row1_col7, #T_7db0f_row1_col8, #T_7db0f_row2_col0, #T_7db0f_row2_col1, #T_7db0f_row2_col2, #T_7db0f_row2_col3, #T_7db0f_row2_col4, #T_7db0f_row2_col5, #T_7db0f_row2_col6, #T_7db0f_row2_col7, #T_7db0f_row2_col8, #T_7db0f_row3_col0, #T_7db0f_row3_col1, #T_7db0f_row3_col2, #T_7db0f_row3_col3, #T_7db0f_row3_col4, #T_7db0f_row3_col5, #T_7db0f_row3_col6, #T_7db0f_row3_col7, #T_7db0f_row3_col8, #T_7db0f_row4_col0, #T_7db0f_row4_col1, #T_7db0f_row4_col2, #T_7db0f_row4_col3, #T_7db0f_row4_col4, #T_7db0f_row4_col5, #T_7db0f_row4_col6, #T_7db0f_row4_col7, #T_7db0f_row4_col8, #T_7db0f_row5_col0, #T_7db0f_row5_col1, #T_7db0f_row5_col2, #T_7db0f_row5_col3, #T_7db0f_row5_col4, #T_7db0f_row5_col5, #T_7db0f_row5_col6, #T_7db0f_row5_col7, #T_7db0f_row5_col8, #T_7db0f_row6_col0, #T_7db0f_row6_col1, #T_7db0f_row6_col2, #T_7db0f_row6_col3, #T_7db0f_row6_col4, #T_7db0f_row6_col5, #T_7db0f_row6_col6, #T_7db0f_row6_col7, #T_7db0f_row6_col8 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7db0f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7db0f_level0_col0\" class=\"col_heading level0 col0\" >Video Name</th>\n",
       "      <th id=\"T_7db0f_level0_col1\" class=\"col_heading level0 col1\" >Original Size</th>\n",
       "      <th id=\"T_7db0f_level0_col2\" class=\"col_heading level0 col2\" >Total Frames</th>\n",
       "      <th id=\"T_7db0f_level0_col3\" class=\"col_heading level0 col3\" >FPS</th>\n",
       "      <th id=\"T_7db0f_level0_col4\" class=\"col_heading level0 col4\" >Total Keyframes</th>\n",
       "      <th id=\"T_7db0f_level0_col5\" class=\"col_heading level0 col5\" >Keyframes Size</th>\n",
       "      <th id=\"T_7db0f_level0_col6\" class=\"col_heading level0 col6\" >Compressed Video Size</th>\n",
       "      <th id=\"T_7db0f_level0_col7\" class=\"col_heading level0 col7\" >Savings Original Video</th>\n",
       "      <th id=\"T_7db0f_level0_col8\" class=\"col_heading level0 col8\" >Savings Keyframes Video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7db0f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7db0f_row0_col0\" class=\"data row0 col0\" >manwalkingsunglassestrim.mp4</td>\n",
       "      <td id=\"T_7db0f_row0_col1\" class=\"data row0 col1\" >1.78 MB</td>\n",
       "      <td id=\"T_7db0f_row0_col2\" class=\"data row0 col2\" >291</td>\n",
       "      <td id=\"T_7db0f_row0_col3\" class=\"data row0 col3\" >29.97 FPS</td>\n",
       "      <td id=\"T_7db0f_row0_col4\" class=\"data row0 col4\" >11</td>\n",
       "      <td id=\"T_7db0f_row0_col5\" class=\"data row0 col5\" >11.55 MB</td>\n",
       "      <td id=\"T_7db0f_row0_col6\" class=\"data row0 col6\" >0.14 MB</td>\n",
       "      <td id=\"T_7db0f_row0_col7\" class=\"data row0 col7\" >99.45 %</td>\n",
       "      <td id=\"T_7db0f_row0_col8\" class=\"data row0 col8\" >44.16 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db0f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7db0f_row1_col0\" class=\"data row1 col0\" >manrunningbeachtrim.mp4</td>\n",
       "      <td id=\"T_7db0f_row1_col1\" class=\"data row1 col1\" >1.79 MB</td>\n",
       "      <td id=\"T_7db0f_row1_col2\" class=\"data row1 col2\" >315</td>\n",
       "      <td id=\"T_7db0f_row1_col3\" class=\"data row1 col3\" >25.00 FPS</td>\n",
       "      <td id=\"T_7db0f_row1_col4\" class=\"data row1 col4\" >10</td>\n",
       "      <td id=\"T_7db0f_row1_col5\" class=\"data row1 col5\" >8.95 MB</td>\n",
       "      <td id=\"T_7db0f_row1_col6\" class=\"data row1 col6\" >0.09 MB</td>\n",
       "      <td id=\"T_7db0f_row1_col7\" class=\"data row1 col7\" >99.67 %</td>\n",
       "      <td id=\"T_7db0f_row1_col8\" class=\"data row1 col8\" >31.02 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db0f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7db0f_row2_col0\" class=\"data row2 col0\" >waterfall.mp4</td>\n",
       "      <td id=\"T_7db0f_row2_col1\" class=\"data row2 col1\" >2.57 MB</td>\n",
       "      <td id=\"T_7db0f_row2_col2\" class=\"data row2 col2\" >288</td>\n",
       "      <td id=\"T_7db0f_row2_col3\" class=\"data row2 col3\" >23.98 FPS</td>\n",
       "      <td id=\"T_7db0f_row2_col4\" class=\"data row2 col4\" >11</td>\n",
       "      <td id=\"T_7db0f_row2_col5\" class=\"data row2 col5\" >27.48 MB</td>\n",
       "      <td id=\"T_7db0f_row2_col6\" class=\"data row2 col6\" >0.50 MB</td>\n",
       "      <td id=\"T_7db0f_row2_col7\" class=\"data row2 col7\" >98.37 %</td>\n",
       "      <td id=\"T_7db0f_row2_col8\" class=\"data row2 col8\" >87.28 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db0f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7db0f_row3_col0\" class=\"data row3 col0\" >waterwavestrim.mp4</td>\n",
       "      <td id=\"T_7db0f_row3_col1\" class=\"data row3 col1\" >23.28 MB</td>\n",
       "      <td id=\"T_7db0f_row3_col2\" class=\"data row3 col2\" >428</td>\n",
       "      <td id=\"T_7db0f_row3_col3\" class=\"data row3 col3\" >25.00 FPS</td>\n",
       "      <td id=\"T_7db0f_row3_col4\" class=\"data row3 col4\" >15</td>\n",
       "      <td id=\"T_7db0f_row3_col5\" class=\"data row3 col5\" >45.54 MB</td>\n",
       "      <td id=\"T_7db0f_row3_col6\" class=\"data row3 col6\" >0.47 MB</td>\n",
       "      <td id=\"T_7db0f_row3_col7\" class=\"data row3 col7\" >99.07 %</td>\n",
       "      <td id=\"T_7db0f_row3_col8\" class=\"data row3 col8\" >88.16 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db0f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7db0f_row4_col0\" class=\"data row4 col0\" >manwalkingcamera1920.mp4</td>\n",
       "      <td id=\"T_7db0f_row4_col1\" class=\"data row4 col1\" >2.86 MB</td>\n",
       "      <td id=\"T_7db0f_row4_col2\" class=\"data row4 col2\" >741</td>\n",
       "      <td id=\"T_7db0f_row4_col3\" class=\"data row4 col3\" >23.98 FPS</td>\n",
       "      <td id=\"T_7db0f_row4_col4\" class=\"data row4 col4\" >27</td>\n",
       "      <td id=\"T_7db0f_row4_col5\" class=\"data row4 col5\" >5.12 MB</td>\n",
       "      <td id=\"T_7db0f_row4_col6\" class=\"data row4 col6\" >0.04 MB</td>\n",
       "      <td id=\"T_7db0f_row4_col7\" class=\"data row4 col7\" >99.78 %</td>\n",
       "      <td id=\"T_7db0f_row4_col8\" class=\"data row4 col8\" >29.98 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db0f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7db0f_row5_col0\" class=\"data row5 col0\" >threecyclysistsbut1920.mp4</td>\n",
       "      <td id=\"T_7db0f_row5_col1\" class=\"data row5 col1\" >5.69 MB</td>\n",
       "      <td id=\"T_7db0f_row5_col2\" class=\"data row5 col2\" >434</td>\n",
       "      <td id=\"T_7db0f_row5_col3\" class=\"data row5 col3\" >29.97 FPS</td>\n",
       "      <td id=\"T_7db0f_row5_col4\" class=\"data row5 col4\" >15</td>\n",
       "      <td id=\"T_7db0f_row5_col5\" class=\"data row5 col5\" >5.64 MB</td>\n",
       "      <td id=\"T_7db0f_row5_col6\" class=\"data row5 col6\" >0.08 MB</td>\n",
       "      <td id=\"T_7db0f_row5_col7\" class=\"data row5 col7\" >99.06 %</td>\n",
       "      <td id=\"T_7db0f_row5_col8\" class=\"data row5 col8\" >65.28 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db0f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7db0f_row6_col0\" class=\"data row6 col0\" >dogrunning1920.mp4</td>\n",
       "      <td id=\"T_7db0f_row6_col1\" class=\"data row6 col1\" >19.84 MB</td>\n",
       "      <td id=\"T_7db0f_row6_col2\" class=\"data row6 col2\" >1382</td>\n",
       "      <td id=\"T_7db0f_row6_col3\" class=\"data row6 col3\" >29.97 FPS</td>\n",
       "      <td id=\"T_7db0f_row6_col4\" class=\"data row6 col4\" >48</td>\n",
       "      <td id=\"T_7db0f_row6_col5\" class=\"data row6 col5\" >15.78 MB</td>\n",
       "      <td id=\"T_7db0f_row6_col6\" class=\"data row6 col6\" >0.10 MB</td>\n",
       "      <td id=\"T_7db0f_row6_col7\" class=\"data row6 col7\" >99.37 %</td>\n",
       "      <td id=\"T_7db0f_row6_col8\" class=\"data row6 col8\" >102.41 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x76f4c8bfc310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT PUT _ in the input directory name. We parse at the _ to get the timestamp of the frame. This can mess up the paths. \n",
    "input_dir = \"/root/VideoReconstruction/input_videos\" # this is the directory where the input videos are stored. \n",
    "output_dir_base = \"/root/VideoReconstruction/outputs/bucketedkeyframes\"\n",
    "smaller_video_output_dir = \"/root/VideoReconstruction/outputs/bucketed_keyframes_video\"\n",
    "\n",
    "!rm -rf $output_dir_base\n",
    "!rm -rf $smaller_video_output_dir\n",
    "\n",
    "os.makedirs(output_dir_base, exist_ok=True)\n",
    "os.makedirs(smaller_video_output_dir, exist_ok=True)\n",
    "\n",
    "videos = os.listdir(input_dir)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "evaluator = FFmpegEvaluator()\n",
    "\n",
    "interims = \"./interims\"\n",
    "os.makedirs(\"./interims\", exist_ok=True)\n",
    "\n",
    "for video in videos:\n",
    "    video_path = os.path.join(input_dir, video)\n",
    "    video_name = os.path.splitext(video)[0]\n",
    "    output_dir = os.path.join(output_dir_base, video_name)\n",
    "    interim_output_path = os.path.join(interims, f\"{video_name}_resized.mp4\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = total_frames / fps\n",
    "    cap.release()\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-i', video_path,  # Input video\n",
    "        '-r', str(fps),  # Set frame rate\n",
    "        '-vf', 'scale=1024:576',  # Resize to 1024x1024\n",
    "        '-c:v', 'libx264',  # Use H.264 codec\n",
    "        '-pix_fmt', 'yuv420p',  # Set pixel format\n",
    "        '-t', str(duration),  \n",
    "        '-y',  # Overwrite existing file\n",
    "        interim_output_path\n",
    "    ]\n",
    "    \n",
    "    process = subprocess.run(command, check=True, capture_output=True)\n",
    "    \n",
    "\n",
    "    video_size_in_bytes = os.path.getsize(interim_output_path)\n",
    "    video_size_in_megabytes = video_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    smartKeyframeDetection(\n",
    "        video_path,\n",
    "        output_dir,\n",
    "        bucket_size_in_frames,\n",
    "        threshold,\n",
    "        output_dir,\n",
    "        minimum_frames_in_between,\n",
    "        maximum_frames_in_between,\n",
    "        segment_fps,\n",
    "        top_k_no_interpolation\n",
    "    )\n",
    "    # get directory size\n",
    "    keyframes = os.listdir(output_dir)\n",
    "    keyframe_count = len(keyframes)\n",
    "\n",
    "    keyframes_dir_size_in_bytes = get_directory_size(output_dir)\n",
    "    keyframes_dir_size_in_megabytes = keyframes_dir_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    compressed_size, savings_from_original_video, savings_from_keyframes_video = evaluator.evaluate_smaller_video(\n",
    "        video_path, smaller_video_output_dir, output_dir)\n",
    "\n",
    "    summary_data.append(\n",
    "        [\n",
    "            video,\n",
    "            f\"{video_size_in_megabytes:.2f} MB\",\n",
    "            total_frames,\n",
    "            f\"{fps:.2f} FPS\",\n",
    "            keyframe_count,\n",
    "            f\"{keyframes_dir_size_in_megabytes:.2f} MB\",\n",
    "            f\"{compressed_size/(1024*1024):.2f} MB\",\n",
    "            f\"{savings_from_original_video:.2f} %\",\n",
    "            f\"{savings_from_keyframes_video:.2f} %\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "headers = [\n",
    "    \"Video Name\",\n",
    "    \"Original Size\",\n",
    "    \"Total Frames\",\n",
    "    \"FPS\",\n",
    "    \"Total Keyframes\",\n",
    "    \"Keyframes Size\",\n",
    "    \"Compressed Video Size\",\n",
    "    \"Savings Original Video\",\n",
    "    \"Savings Keyframes Video\"\n",
    "]\n",
    "# print(tabulate(summary_data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "df = pd.DataFrame(summary_data, columns=headers)\n",
    "output_csv_path = \"/root/VideoReconstruction/logs/bandwidth_savings_data.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "shutil.rmtree(interims)\n",
    "\n",
    "df.style.set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('font-weight', 'bold'), ('background-color', '#000000')]}]\n",
    ").set_properties(**{'text-align': 'center'})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Settings for Image-to-Video Interpolation:\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Parameter                    | Value                                         |\n",
      "+==============================+===============================================+\n",
      "| Model                        | stabilityai/stable-video-diffusion-img2vid-xt |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Output Directory             | /root/VideoReconstruction/our_results         |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Resize Specifications        | 1024 x 576                                    |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| FPS                          | 15                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Frame Duration (ms)          | 142                                           |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Inference Steps              | 20                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Random Seed                  | 42                                            |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Saving interpolated video as | mp4 video file                                |\n",
      "+------------------------------+-----------------------------------------------+\n",
      "| Number of Frames             | 7                                             |\n",
      "+------------------------------+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INPUT_DIR is the directory from which the keyframes will be read\n",
    "# DO NOT PUT _ in the input directory name. We parse at the _ to get the timestamp of the frame. This can mess up the paths. \n",
    "\n",
    "INPUT_DIR = \"/root/VideoReconstruction/outputs/bucketedkeyframes\"\n",
    "OUT_DIR = \"/root/VideoReconstruction/our_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_DIR = \"/root/VideoReconstruction/svd/checkpoints/svd_reverse_motion_with_attnflip/svd_reverse_motion_with_attnflip/unet\"  # path for docker image\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "# Model options and selection. maybe we cam add more models later for this\n",
    "models_to_try = [\"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "                 \"stabilityai/stable-video-diffusion-img2vid-xt-1-1\"]\n",
    "MODEL_NAME = models_to_try[0]\n",
    "\n",
    "# Noise injection parameters\n",
    "noise_injection_steps = 2\n",
    "noise_injection_ratio = 0.5\n",
    "\n",
    "# if this is .gif, the output will be a gif otherwise it will be a video\n",
    "interpolation_extension = \".mp4\"\n",
    "final_extension = \".mp4\"\n",
    "\n",
    "# Resize specifications, frame rate, and duration etc\n",
    "resize_specs = (1024, 576)\n",
    "fps = 15 # the frame rate of the output video\n",
    "duration = 142  # the duration for which each frame is displayed in the vid\n",
    "inference_steps = 20 # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\n",
    "seed = 42\n",
    "decode_chunk_size = 10 # controls how many frames are decoded at a time from the latent representations during the video generation process. A smaller value will use less memory but may be slower. The higher the chunk size, the higher the temporal consistency between frames, but also the higher the memory consumption. By default, the decoder will decode all frames at once for maximal quality. Reduce `decode_chunk_size` to reduce memory usage.\n",
    "num_frames = 7 #  The number of frames to decode at a time. We decide these dynamically based on the timestamps of the frames.\n",
    "\n",
    "# ===========================\n",
    "# Display Configuration\n",
    "# ===========================\n",
    "\n",
    "settings = [\n",
    "    [\"Model\", MODEL_NAME],\n",
    "    [\"Output Directory\", OUT_DIR],\n",
    "    [\"Resize Specifications\", f\"{resize_specs[0]} x {resize_specs[1]}\"],\n",
    "    [\"FPS\", fps],\n",
    "    [\"Frame Duration (ms)\", duration],\n",
    "    [\"Inference Steps\", inference_steps],\n",
    "    [\"Random Seed\", seed],\n",
    "    [\"Saving interpolated video as\", \"gif file\" if interpolation_extension ==\n",
    "        \".gif\" else \"mp4 video file\"],\n",
    "    [\"Number of Frames\", num_frames],\n",
    "]\n",
    "\n",
    "print(\"\\nSettings for Image-to-Video Interpolation:\")\n",
    "print(tabulate(settings, headers=[\"Parameter\", \"Value\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looped interpolation\n",
    "\n",
    "Here, we loop through all the videos in the input folder, generate keyframes for them and then interpolate between the keyframes to get the final video. This is done for all the videos in the input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m video_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define the full path to the video file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43minput_dir\u001b[49m, filename)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     generate_all_interpolations(checkpoint_dir\u001b[38;5;241m=\u001b[39mCHECKPOINT_DIR, input_sub_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(INPUT_DIR, video_name), output_dir\u001b[38;5;241m=\u001b[39mOUT_DIR, model_name\u001b[38;5;241m=\u001b[39mMODEL_NAME, resize_specs\u001b[38;5;241m=\u001b[39mresize_specs, fps\u001b[38;5;241m=\u001b[39mfps, duration\u001b[38;5;241m=\u001b[39mduration, extension\u001b[38;5;241m=\u001b[39minterpolation_extension,\n\u001b[1;32m     11\u001b[0m                                 seed\u001b[38;5;241m=\u001b[39mseed, inference_steps\u001b[38;5;241m=\u001b[39minference_steps, noise_injection_steps\u001b[38;5;241m=\u001b[39mnoise_injection_steps, noise_injection_ratio\u001b[38;5;241m=\u001b[39mnoise_injection_ratio, decode_chunk_size\u001b[38;5;241m=\u001b[39mdecode_chunk_size, num_frames\u001b[38;5;241m=\u001b[39mnum_frames, video_name\u001b[38;5;241m=\u001b[39mvideo_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop over each file in the input directory\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    start_time = time.time()\n",
    "    video_name = os.path.splitext(filename)[0] # get the base name of the file without the extension\n",
    "    video_path = os.path.join(INPUT_DIR, filename)\n",
    "    \n",
    "    try:\n",
    "        generate_all_interpolations(checkpoint_dir=CHECKPOINT_DIR, input_sub_dir=os.path.join(INPUT_DIR, video_name), output_dir=OUT_DIR, model_name=MODEL_NAME, resize_specs=resize_specs, fps=fps, duration=duration, extension=interpolation_extension,\n",
    "                                    seed=seed, inference_steps=inference_steps, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, decode_chunk_size=decode_chunk_size, num_frames=num_frames, video_name=video_name)\n",
    "        stitch_video_segments(intermediate_videos_dir=os.path.join(OUT_DIR, f\"interm_videos_{video_name}\"), output_path=os.path.join(\n",
    "            OUT_DIR, f\"{video_name}_interpolated{final_extension}\"), fps=fps)\n",
    "    except Exception as e:\n",
    "        print(\"Error while generate_all_interpolations\")\n",
    "        print(f\"Error: {e}\")\n",
    "        # i am doing this cuz it is annoying to get out of memory erros every time there is an error. so we will kill the process after it is done/error thrown\n",
    "        print(\"Killing the kernel processes\")\n",
    "        env_name = os.path.basename(sys.prefix)\n",
    "        subprocess.call(\n",
    "            f\"nvidia-smi | grep {env_name} | awk '{{print $5}}' | xargs -I {{}} kill -9 {{}}\",\n",
    "            shell=True\n",
    "        )\n",
    "        raise e\n",
    "    \n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time for {video_name}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inidvidual Video Processing\n",
    "\n",
    "You can uncomment the below cell to run the interpolation for a single video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For /root/VideoReconstruction/input_videos/waterfall.mp4, selected 2 frames, and inserted 9 frames.\n",
      "Found 11 frames in /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame0_0.0000.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame37_1.5432.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8028aa02204fcebc43514a3a162942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8413b915a4334b41876f1a980320434c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 23 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_0.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_0.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame37_1.5432.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame73_3.0447.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2102ce88da474fa15dae5d99848517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22db76dfc954ffbae6ec9d00fe1cf54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 23 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_1.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_1.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame73_3.0447.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame97_4.0457.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a303c00a4bf84266982b76df01b2ca05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dcd94bd7ec46e589b4b895476ba388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 15 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_2.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_2.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame97_4.0457.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame121_5.0467.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d06255209c423485edab222491695d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d66cd9f1aba4de1af24500a27f696e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 15 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_3.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_3.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame121_5.0467.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame157_6.5482.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d8e5e951a5489e86c490184a833200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db11a92733994965b3eb7685fc10d971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 23 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_4.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_4.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame157_6.5482.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame181_7.5492.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905d96996fd84af3b58c098e0d8ff902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef656b0707ee4c4983cf3fd953a083d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 15 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_5.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_5.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame181_7.5492.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame205_8.5502.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeb94679e954b72856d9658cef42e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac92a193ff6944cf93f1d1c825231db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 15 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_6.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_6.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame205_8.5502.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame229_9.5512.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451102b03ad945c39e0480b6e79ffbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1443bb54fc4a41afb452a6be3eba6c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 15 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_7.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_7.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame229_9.5512.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame259_10.8025.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dc4f159d824059b5f43a374d9838d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0041380cd5b343a7aacb2f353a45834d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 19 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_8.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_8.mp4\n",
      "Interpolating between frames /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame259_10.8025.jpg and /root/VideoReconstruction/outputs/bucketedkeyframes/waterfall/frame287_11.9703.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82af436f00a24391be04e36bf74b6e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283d96753de747a399eebb20956fab85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 18 frames to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_9.mp4 as video\n",
      "Interpolated video saved to /root/VideoReconstruction/our_results/interm_videos_waterfall/segment_9.mp4\n",
      "Found 10 segments to stitch.\n",
      "Moviepy - Building video /root/VideoReconstruction/our_results/waterfall_interpolated.mp4.\n",
      "Moviepy - Writing video /root/VideoReconstruction/our_results/waterfall_interpolated.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/VideoReconstruction/our_results/waterfall_interpolated.mp4\n",
      "Final video saved to /root/VideoReconstruction/our_results/waterfall_interpolated.mp4\n"
     ]
    }
   ],
   "source": [
    "# the input_dir should be the path to input video\n",
    "# DO NOT PUT _ in the input directory name. We parse at the _ to get the timestamp of the frame. This can mess up the paths. \n",
    "\n",
    "input_dir = \"/root/VideoReconstruction/input_videos/waterfall.mp4\"\n",
    "output_keyframes_dir_base = \"/root/VideoReconstruction/outputs/bucketedkeyframes\"\n",
    "output_video_dir = \"/root/VideoReconstruction/our_results/\"\n",
    "video_name = os.path.splitext(os.path.basename(input_dir))[0]\n",
    "os.makedirs(output_keyframes_dir_base, exist_ok=True)\n",
    "output_keyframes_dir = os.path.join(output_keyframes_dir_base, video_name)\n",
    "\n",
    "smartKeyframeDetection(input_dir, \"\", bucket_size_in_frames, threshold, output_keyframes_dir_base, minimum_frames_in_between, maximum_frames_in_between, segment_fps, top_k_no_interpolation)\n",
    "\n",
    "try:\n",
    "    generate_all_interpolations(checkpoint_dir=CHECKPOINT_DIR, input_sub_dir=output_keyframes_dir, output_dir=output_video_dir, model_name=MODEL_NAME, resize_specs=resize_specs, fps=fps, duration=duration, extension=interpolation_extension,\n",
    "                    seed=seed, inference_steps=inference_steps, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, decode_chunk_size=decode_chunk_size, num_frames=num_frames)\n",
    "    stitch_video_segments(intermediate_videos_dir=os.path.join(OUT_DIR, f\"interm_videos_{video_name}\"), output_path=os.path.join(\n",
    "            OUT_DIR, f\"{video_name}_interpolated{final_extension}\"), fps=fps)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    # i am doing this cuz it is annoying to get out of memory erros every time there is an error. so we will kill the process after it is done/error thrown\n",
    "    print(\"Killing the kernel processes\")\n",
    "    env_name = os.path.basename(sys.prefix)\n",
    "    subprocess.call(\n",
    "    f\"nvidia-smi | grep {env_name} | awk '{{print $5}}' | xargs -I {{}} kill -9 {{}}\",\n",
    "    shell=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can kill the kernel processes after we are done with the interpolation to clear up GPU memory\n",
    "\n",
    "!nvidia-smi | grep $(conda env list | grep '*' | awk '{print $1}') | awk '{print $5}' | xargs -I {} kill -9 {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate the generated video against the original video using several methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the paths and other hyper parameters of the two videos to compare against each other.\n",
    "# generated_video = \"office_dude/1.mp4\"\n",
    "# # generated_video = \"our_results_strawman/car_drive_interpolated.mp4\"\n",
    "# original_video = \"office_dude/2.mp4\"\n",
    "# # original_video = \"input/car_drive.mp4\"\n",
    "# frame_width = 1024\n",
    "# frame_height = 576\n",
    "# max_frames = 300\n",
    "\n",
    "# # make directories for storing the frames of both of these videos if they dont exist already\n",
    "# frames_dir = \"frames\"\n",
    "# if os.path.exists(frames_dir):\n",
    "#     shutil.rmtree(frames_dir)\n",
    "# generated_video_frame_dir = os.path.join(frames_dir, \"input\")\n",
    "# original_video_frame_dir = os.path.join(frames_dir, \"output\")\n",
    "# if not os.path.exists(frames_dir):\n",
    "#     os.makedirs(frames_dir)\n",
    "# if not os.path.exists(generated_video_frame_dir):\n",
    "#     os.makedirs(generated_video_frame_dir)\n",
    "# if not os.path.exists(original_video_frame_dir):\n",
    "#     os.makedirs(original_video_frame_dir)\n",
    "\n",
    "# original_video_dir = \"/VideoReconstruction/input\"\n",
    "# generated_video_dir = \"/VideoReconstruction/our_results_strawman\"\n",
    "\n",
    "# # # get file basename form original video dir\n",
    "# # for file in os.listdir(original_video_dir):\n",
    "# #     if file.endswith(\".mp4\"):\n",
    "# #         filename = os.path.splitext(file)[0]\n",
    "# #         print(f\"Processing {filename}\")\n",
    "# #         generated_video = f\"{generated_video_dir}/{filename}_interpolated.mp4\"\n",
    "# #         original_video = f\"{original_video_dir}/{filename}.mp4\"\n",
    "# #         # break\n",
    "# #         # break both the videos down into frames\n",
    "# process_and_extract_frames(input_video_path=generated_video, frames_output_dir=generated_video_frame_dir,\n",
    "#                            new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "\n",
    "# process_and_extract_frames(input_video_path=original_video, frames_output_dir=original_video_frame_dir,\n",
    "#                            new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "# # Pass it the directories with the frames of original and generated video to retrieve the perceptual loss across each frame\n",
    "# calculate_lpips_distance(dir0=generated_video_frame_dir,\n",
    "#                          dir1=original_video_frame_dir, use_gpu=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-0-27-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
