{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for interpolation of Keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add pipeline for keyframe generation here as well (IMP)\n",
    "# DONE : Stitch the final video together after running interpolation on keyframe pairs\n",
    "# DONE : Figure out why the kernel starts shaking crying throwing up when I run the interpolation code. (IMP)\n",
    "# Basically it dies at the last iteration for some reason. If i move the pipe out to save the time for loading it for every interpolation, it dies at second iteration. My thinking is that it is related to the cache/pipe/scehular being deleted/emtpied at wrong time/or maybe just that memeory becomes full but honeslty the latter dont be making sense cuz of the variation in wehn it does as described above\n",
    "# TODO : Compare across different models (LATER)\n",
    "# TODO : Add a function for displaying graphs (LATER)\n",
    "# TODO : Compare across settings. (LATER)\n",
    "# TODO : Add a function for calculating the PSNR and SSIM or whatever similarity metric we decide to use (IMP)\n",
    "# TODO : Add the requirements.txt for this notebook (LATER)\n",
    "# TODO : Make the markdown cells more informative, add description of what each cell does (LATER)\n",
    "\n",
    "# TODO : explore algorithmic interpolation\n",
    "# TODO : Add frame generation to the pipeline as well\n",
    "# TODO : Color smoothen/correct final video \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > our_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This code uses the `diffusers-0-27-0` environment\n",
    "<br>\n",
    "<span style=\"color:red\">**NOTE:** Define all imports and install/shell commands here</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# add install or other terminal commands here\n",
    "# %pip install numpy matplotlib opencv-python scikit-image scikit-video pillow\n",
    "# %pip install tabulate\n",
    "# %conda install -c conda-forge ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "import sys\n",
    "from KeyFrameDetector.key_frame_detector import keyframeDetectionByChunks\n",
    "from KeyFrameDetector.key_frame_detector import smartKeyframeDetection\n",
    "from utils.extract_frames import strawman_frame_extraction\n",
    "from utils.katna import num_frames_keyframe\n",
    "from utils.perceptual_similarity import calculate_lpips_distance\n",
    "from utils.extract_frames import process_and_extract_frames\n",
    "import numpy as np\n",
    "from moviepy.editor import concatenate_videoclips, VideoFileClip\n",
    "from tabulate import tabulate\n",
    "from svd.attn_ctrl.attention_control import (\n",
    "    AttentionStore,\n",
    "    register_temporal_self_attention_control,\n",
    "    register_temporal_self_attention_flip_control,\n",
    ")\n",
    "from svd.custom_diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler\n",
    "from svd.custom_diffusers.pipelines.pipeline_frame_interpolation_with_noise_injection import FrameInterpolationWithNoiseInjectionPipeline\n",
    "from diffusers import UNetSpatioTemporalConditionModel\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "from utils.ffmpeg_evaluator import FFmpegEvaluator\n",
    "\n",
    "os.chdir(\"/root/VideoReconstruction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "There are two functuions in this pipeline:\n",
    "\n",
    "1. `interpolate_keyframes` - This function takes in 2 keyframes and interpolates the keyframes to get the intermediate keyframes.\n",
    "2. `generate_video` - This function uses a list of keyframes and interpolates between every two consecutive keyframes to get a list of intermediate video segments which are then concatenated to get the final video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "The `run_interpolation` function interpolates between two keyframes and saves the output video to the specifed path.\n",
    "\n",
    "The  `generate_all_interpolations` function generates all the intermediate keyframes between every two consecutive keyframes in an input directory with all the keyframes in it saving all the output videos to the output path.\n",
    "\n",
    "The `stitch_video_segments` function stitches the video segments to get the final video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interpolation(checkpoint_dir, frame1_path, frame2_path, out_path, resize_specs, fps, pretrained_model_name_or_path, duration, num_frames, seed=42, num_inference_steps=50, weighted_average=False, noise_injection_steps=0, noise_injection_ratio=0.5, decode_chunk_size=8, device=\"cuda:0\"):\n",
    "    \"\"\"\n",
    "    Run key frame interpolation between two frames using a pretrained model and noise injection. It saves the interpolated video or gif to the specified output path.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_dir (str): Directory containing the checkpoint for the fine-tuned UNet model.\n",
    "        frame1_path (str): Path to the first frame image.\n",
    "        frame2_path (str): Path to the second frame image.\n",
    "        out_path (str): Path to save the output interpolated video or gif.\n",
    "        resize_specs (tuple): Tuple specifying the resize dimensions (width, height) for the input frames.\n",
    "        fps (int): Frames per second for the output video.\n",
    "        pretrained_model_name_or_path (str): Path or name of the pretrained model.\n",
    "        duration (int): Duration for which each frame will be displayed in the gif (in milliseconds).\n",
    "        num_frames (int): Number of frames to interpolate between the two input frames.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        num_inference_steps (int, optional): Number of inference steps for the interpolation. Defaults to 50.\n",
    "        weighted_average (bool, optional): Whether to use weighted average during interpolation. Defaults to False. \n",
    "        True: Produces a video with a gradual shift from image1 to image2. This can give the effect of morphing or smooth interpolation between two images.\n",
    "        False: Maintains an equal influence of both images across all frames. This results in a more consistent combination of features from both image1 and image2.\n",
    "\n",
    "        noise_injection_steps (int, optional): Number of steps for noise injection. Defaults to 0.\n",
    "        noise_injection_ratio (float, optional): Ratio of noise injection. Defaults to 0.5.\n",
    "        decode_chunk_size (int, optional): Chunk size for decoding. Defaults to 8. controls how many frames are decoded at a time from the latent representations during the video generation process.\n",
    "\n",
    "\n",
    "        device (str, optional): Device to run the interpolation on. Defaults to \"cuda:0\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load noise scheduler and pipeline\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "    pipe = FrameInterpolationWithNoiseInjectionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path, scheduler=noise_scheduler, variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Set up UNet model for fine-tuning and load state dicts\n",
    "\n",
    "    ref_unet = pipe.ori_unet\n",
    "    state_dict = pipe.unet.state_dict()\n",
    "\n",
    "    # Compute delta weights\n",
    "    finetuned_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        checkpoint_dir, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "\n",
    "    ori_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"unet\", variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Apply delta to state dict for specific layers\n",
    "    finetuned_state_dict = finetuned_unet.state_dict()\n",
    "    ori_state_dict = ori_unet.state_dict()\n",
    "    for name, param in finetuned_state_dict.items():\n",
    "        if \"temporal_transformer_blocks.0.attn1.to_v\" in name or \"temporal_transformer_blocks.0.attn1.to_out.0\" in name:\n",
    "            delta_w = param - ori_state_dict[name]\n",
    "            state_dict[name] = state_dict[name] + delta_w\n",
    "    pipe.unet.load_state_dict(state_dict)\n",
    "\n",
    "    # Setup attention controllers\n",
    "    controller_ref = AttentionStore()\n",
    "    register_temporal_self_attention_control(ref_unet, controller_ref)\n",
    "\n",
    "    controller = AttentionStore()\n",
    "    register_temporal_self_attention_flip_control(\n",
    "        pipe.unet, controller, controller_ref)\n",
    "\n",
    "    # Move pipeline to specified device\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "    # Set random seed\n",
    "    generator = torch.Generator(device=device)\n",
    "    if seed is not None:\n",
    "        generator = generator.manual_seed(seed)\n",
    "\n",
    "    # Load and resize frames\n",
    "    frame1 = load_image(frame1_path).resize(resize_specs)\n",
    "    frame2 = load_image(frame2_path).resize(resize_specs)\n",
    "\n",
    "    # Perform inference\n",
    "    # print(f\"frame 1 path \", frame1_path)\n",
    "    # print(f\"frame 2 path \", frame2_path)\n",
    "\n",
    "    timestamp_f1 = frame1_path.split(\"_\")[1]\n",
    "    timestamp_f2 = frame2_path.split(\"_\")[1]\n",
    "    # print(\n",
    "    #     f\"after splitting the list for f1 is {timestamp_f1} and for f2 is {timestamp_f2}\")\n",
    "    timestamp_f1 = timestamp_f1.split(\".jpg\")[0]\n",
    "    timestamp_f2 = timestamp_f2.split(\".jpg\")[0]\n",
    "    # print(f\"timestamp_f1 : {timestamp_f1}\")\n",
    "    # print(f\"timestamp_f2 : {timestamp_f2}\")\n",
    "\n",
    "    num_frames = int(\n",
    "        np.round((float(timestamp_f2) - float(timestamp_f1)) * fps))\n",
    "\n",
    "    if num_frames > 24:\n",
    "        num_frames = 24\n",
    "    elif num_frames < 2:\n",
    "        num_frames = 2\n",
    "\n",
    "    # print(f\"num_frames : {num_frames}\")\n",
    "\n",
    "    frames = pipe(image1=frame1, image2=frame2, num_inference_steps=num_inference_steps, generator=generator, weighted_average=weighted_average,\n",
    "                  noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, num_frames=num_frames).frames[0]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    # duration = the time  for whicch each frame will be displayed in the gif\n",
    "    if out_path.endswith(\".gif\"):\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as gif\")\n",
    "        frames[0].save(out_path, save_all=True,\n",
    "                       append_images=frames[1:], duration=duration, loop=0)\n",
    "    else:\n",
    "        print(f\"Saving {len(frames)} frames to {out_path} as video\")\n",
    "        export_to_video(frames, out_path, fps=fps)\n",
    "\n",
    "    print(f\"Interpolated video saved to {out_path}\")\n",
    "\n",
    "    # Free GPU memory after inference\n",
    "    del controller, controller_ref, ori_unet, finetuned_unet\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_interpolations(checkpoint_dir, input_sub_dir, output_dir, model_name, num_frames, decode_chunk_size, extension=\".gif\", resize_specs=(1024, 576), fps=7, duration=142, seed=42, inference_steps=20, noise_injection_steps=2, noise_injection_ratio=0.5, device=\"cuda:0\", video_name=None):\n",
    "    \"\"\"\n",
    "    Generates interpolated videos from key frames in the input directory.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_dir (str): Directory containing the model checkpoints.\n",
    "        input_sub_dir (str): Directory containing the input frames.\n",
    "        output_dir (str): Directory to save the output videos.\n",
    "        model_name (str): Name of the pre-trained model to use for interpolation.\n",
    "        num_frames (int): Number of frames to generate between each pair of key frames.\n",
    "        decode_chunk_size (int): Size of the chunk to decode.\n",
    "        extension (str, optional): Extension of the output video files. Defaults to \".gif\".\n",
    "        resize_specs (tuple, optional): Tuple specifying the width and height to resize the frames. Defaults to (1024, 576).\n",
    "        fps (int, optional): Frames per second for the output video. Defaults to 7.\n",
    "        duration (int, optional): Duration of the output video in seconds. Defaults to 142.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        inference_steps (int, optional): Number of inference steps for the model. Defaults to 20.\n",
    "        noise_injection_steps (int, optional): Number of steps to inject noise during inference. Defaults to 2.\n",
    "        noise_injection_ratio (float, optional): Ratio of noise to inject during inference. Defaults to 0.5.\n",
    "        device (str, optional): Device to run the model on. Defaults to \"cuda:0\".\n",
    "        video_name (str, optional): Name of the video. If None, it will be derived from the input_sub_dir. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # video_name = os.path.basename(input_sub_dir)\n",
    "\n",
    "    # print(f\"Generating video for {video_name}\")\n",
    "    video_name = os.path.basename(input_sub_dir)\n",
    "\n",
    "    intermediate_videos_dir = os.path.join(\n",
    "        output_dir, f\"interm_videos_{video_name}\")\n",
    "    os.makedirs(intermediate_videos_dir, exist_ok=True)\n",
    "\n",
    "    frames = sorted([os.path.join(input_sub_dir, f) for f in os.listdir(\n",
    "        input_sub_dir) if f.endswith((\".png\", \".jpeg\", \".jpg\"))])\n",
    "\n",
    "    print(f\"Found {len(frames)} frames in {input_sub_dir}\")\n",
    "    if len(frames) < 2:\n",
    "        print(f\"Skipping {video_name} as it has less than 2 frames\")\n",
    "        return\n",
    "\n",
    "    print(f\"Frames : {frames}\")\n",
    "    frames = sorted(frames, key=lambda x: float(\n",
    "        x.split(\"_\")[-1].replace(\".jpg\", \"\")))\n",
    "    print(f\"Frames : {frames}\")\n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1_path = frames[i]\n",
    "        frame2_path = frames[i + 1]\n",
    "\n",
    "        print(f\"Interpolating between frames {frame1_path} and {frame2_path}\")\n",
    "\n",
    "        if \"<s>\" in frame1_path:\n",
    "            print(f\"Skipping {frame1_path} as its entrie bucket is already saved\")\n",
    "            continue\n",
    "        \n",
    "        segment_output_path = os.path.join(\n",
    "            intermediate_videos_dir, f\"segment_{i}{extension}\")\n",
    "\n",
    "        run_interpolation(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            frame1_path=frame1_path,\n",
    "            frame2_path=frame2_path,\n",
    "            out_path=segment_output_path,\n",
    "            resize_specs=resize_specs,\n",
    "            fps=fps,\n",
    "            pretrained_model_name_or_path=model_name,\n",
    "            duration=duration,\n",
    "            seed=seed,\n",
    "            num_inference_steps=inference_steps,\n",
    "            noise_injection_ratio=noise_injection_ratio,\n",
    "            noise_injection_steps=noise_injection_steps,\n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_video_segments(intermediate_videos_dir, output_path, fps):\n",
    "    \"\"\"\n",
    "    Stitches together video segments from a specified directory into a single video file.\n",
    "\n",
    "    Args:\n",
    "        intermediate_videos_dir (str): The directory containing the video segments to be stitched together.\n",
    "        output_path (str): The file path where the final stitched video will be saved.\n",
    "        fps (int): The frames per second (fps) rate for the final video.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Prints:\n",
    "        The number of segments found and a confirmation message when the final video is saved.\n",
    "    \"\"\"\n",
    "    segment_paths = sorted([os.path.join(intermediate_videos_dir, f)\n",
    "                           for f in os.listdir(intermediate_videos_dir)])\n",
    "\n",
    "    print(f\"Found {len(segment_paths)} segments to stitch.\")\n",
    "    clips = [VideoFileClip(segment).set_fps(fps) for segment in segment_paths]\n",
    "    final_video = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_video.write_videofile(output_path, fps=fps)\n",
    "    print(f\"Final video saved to {output_path}\")\n",
    "\n",
    "    for clip in clips:\n",
    "        clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyframe Generation and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyframe size savings\n",
    "\n",
    "\n",
    "This part of the code takes all the videos in input folder and generates keyframes for each video and stores them in the output folder. It also benchmarks the sizes of the original videos and videos from keyframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"/root/VideoReconstruction/input_resized\"\n",
    "output_dir_base = \"/root/VideoReconstruction/outputs/bucketed_keyframes\"\n",
    "smaller_video_output_dir = \"/root/VideoReconstruction/outputs/bucketed_keyframes_video\"\n",
    "\n",
    "os.makedirs(output_dir_base, exist_ok=True)\n",
    "os.makedirs(smaller_video_output_dir, exist_ok=True)\n",
    "\n",
    "videos = os.listdir(input_dir)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "evaluator = FFmpegEvaluator()\n",
    "\n",
    "for video in videos:\n",
    "    video_path = os.path.join(input_dir, video)\n",
    "    video_name = os.path.splitext(video)[0]\n",
    "    output_dir = os.path.join(output_dir_base, video_name)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "\n",
    "    video_size_in_bytes = os.path.getsize(video_path)\n",
    "    video_size_in_megabytes = video_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    keyframes = smartKeyframeDetection(video_path, output_dir, threshold = 0.3, bucket_size_in_frames = 80, minimum_frames_between=20, maximum_frames_between = 30)\n",
    "    # keyframes = keyframeDetectionByChunks(video_path, output_dir, number_frames_per_bucket=60, top_k=5, verbose=False, minimum_frames_between=20, maximum_frames_between=30)\n",
    "    keyframe_count = len(keyframes)\n",
    "\n",
    "    keyframes_dir_size_in_bytes = get_directory_size(output_dir)\n",
    "    keyframes_dir_size_in_megabytes = keyframes_dir_size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    output_dir = f\"{output_dir}/keyFrames\"\n",
    "    compressed_size, savings_from_original_video, savings_from_keyframes_video = evaluator.evaluate_smaller_video(\n",
    "        video_path, smaller_video_output_dir, output_dir)\n",
    "\n",
    "    summary_data.append(\n",
    "        [\n",
    "            video,\n",
    "            f\"{video_size_in_megabytes:.2f} MB\",\n",
    "            total_frames,\n",
    "            f\"{fps:.2f} FPS\",\n",
    "            keyframe_count,\n",
    "            f\"{keyframes_dir_size_in_megabytes:.2f} MB\",\n",
    "            f\"{compressed_size/(1024*1024):.2f} MB\",\n",
    "            f\"{savings_from_original_video:.2f} %\",\n",
    "            f\"{savings_from_keyframes_video:.2f} %\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "headers = [\n",
    "    \"Video Name\",\n",
    "    \"Original Size\",\n",
    "    \"Total Frames\",\n",
    "    \"FPS\",\n",
    "    \"Total Keyframes\",\n",
    "    \"Keyframes Size\",\n",
    "    \"Compressed Video Size\",\n",
    "    \"Savings Original Video\",\n",
    "    \"Savings Keyframes Video\"\n",
    "]\n",
    "print(tabulate(summary_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/root/VideoReconstruction/outputs/bucketedkeyframes/beach\"\n",
    "# output_dir = \"/VideoReconstruction/interim\"\n",
    "output_dir = \"/root/VideoReconstruction/interim/keyFrames\"\n",
    "# ============ ONLY THE FOLLOWING TWO PATHS ARE BEING USED IN *INTERPOLATION* ============\n",
    "# INPUT_DIR is the directory from which the keyframes will be read\n",
    "INPUT_DIR = \"/root/VideoReconstruction/outputs/bucketedkeyframes\"\n",
    "input_subdir_name = \"beach\"\n",
    "# OUT_DIR + interim_videos_<vidname> is the directory where the interpolated segments will be saved\n",
    "# OUT_DIR + <vidname>_interpolate is the filename of the final stitched video\n",
    "OUT_DIR = \"/root/VideoReconstruction/our_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "threshold = 0.3\n",
    "\n",
    "\n",
    "# Model options and selection. maybe we cam add more models later for this\n",
    "models_to_try = [\"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "                 \"stabilityai/stable-video-diffusion-img2vid-xt-1-1\"]\n",
    "MODEL_NAME = models_to_try[0]\n",
    "\n",
    "# Noise injection parameters\n",
    "noise_injection_steps = 2\n",
    "noise_injection_ratio = 0.5\n",
    "\n",
    "# All Directory paths are defined here\n",
    "# CHECKPOINT_DIR = \"/home/iml2/interpolation/svd_keyframe_interpolation/checkpoints/svd_reverse_motion_with_attnflip/svd_reverse_motion_with_attnflip/unet\" #path for iml2\n",
    "CHECKPOINT_DIR = \"/root/VideoReconstruction/svd/checkpoints/svd_reverse_motion_with_attnflip/svd_reverse_motion_with_attnflip/unet\"  # path for docker image\n",
    "# if this is .gif, the output will be a gif otherwise it will be a video\n",
    "interpolation_extension = \".mp4\"\n",
    "final_extension = \".mp4\"\n",
    "\n",
    "# Resize specifications, frame rate, and duration etc\n",
    "resize_specs = (1024, 576)\n",
    "fps = 15\n",
    "duration = 142  # the duration for which each frame is displayed in the vid\n",
    "inference_steps = 20\n",
    "seed = 42\n",
    "decode_chunk_size = 6\n",
    "num_frames = 7\n",
    "\n",
    "\n",
    "# INPUT_DIR = \"/home/iml2/manframes\"\n",
    "# INPUT_DIR = \"/VideoReconstruction/input\"\n",
    "# input_subdir_name = \"horses\"\n",
    "input_subdir_path = os.path.join(INPUT_DIR, input_subdir_name)\n",
    "\n",
    "# # the katana measure to get frames but they come without timestamps\n",
    "# num_frames_keyframe(video_path, output_dir, num_frames)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Display Configuration\n",
    "# ===========================\n",
    "\n",
    "settings = [\n",
    "    [\"Model\", MODEL_NAME],\n",
    "    [\"Output Directory\", OUT_DIR],\n",
    "    [\"Input Directory\", input_subdir_name],\n",
    "    [\"Resize Specifications\", f\"{resize_specs[0]} x {resize_specs[1]}\"],\n",
    "    [\"FPS\", fps],\n",
    "    [\"Frame Duration (ms)\", duration],\n",
    "    [\"Inference Steps\", inference_steps],\n",
    "    [\"Random Seed\", seed],\n",
    "    [\"Saving interpolated video as\", \"gif file\" if interpolation_extension ==\n",
    "        \".gif\" else \"mp4 video file\"],\n",
    "    [\"Number of Frames\", num_frames],\n",
    "]\n",
    "\n",
    "print(\"\\nSettings for Image-to-Video Interpolation:\")\n",
    "print(tabulate(settings, headers=[\"Parameter\", \"Value\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we loop through all the videos in the input folder, generate keyframes for them and then interpolate between the keyframes to get the final video. This is done for all the videos in the input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each file in the input directory\n",
    "for filename in os.listdir(\"./input_videos/\"):\n",
    "    start_time = time.time()\n",
    "    # Get the base name without the extension \n",
    "    video_name = os.path.splitext(filename)[0]\n",
    "    # Define the full path to the video file\n",
    "    video_path = os.path.join(input_dir, filename)\n",
    "    try:\n",
    "        # print(\"input_sub_dir: \", os.path.join(INPUT_DIR, video_name))\n",
    "        # print(\"OUT_DIR: \", OUT_DIR)\n",
    "        # continue\n",
    "        generate_all_interpolations(checkpoint_dir=CHECKPOINT_DIR, input_sub_dir=os.path.join(INPUT_DIR, video_name), output_dir=OUT_DIR, model_name=MODEL_NAME, resize_specs=resize_specs, fps=fps, duration=duration, extension=interpolation_extension,\n",
    "                                    seed=seed, inference_steps=inference_steps, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, decode_chunk_size=decode_chunk_size, num_frames=num_frames, video_name=video_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error while generate_all_interpolations\")\n",
    "        print(f\"Error: {e}\")\n",
    "        # i am doing this cuz it is annoying to get out of memory erros every time there is an error. so we will kill the process after it is done/error thrown\n",
    "        print(\"Killing the kernel processes\")\n",
    "        env_name = os.path.basename(sys.prefix)\n",
    "        subprocess.call(\n",
    "            f\"nvidia-smi | grep {env_name} | awk '{{print $5}}' | xargs -I {{}} kill -9 {{}}\",\n",
    "            shell=True\n",
    "        )\n",
    "        raise e\n",
    "    \n",
    "    try:\n",
    "        stitch_video_segments(intermediate_videos_dir=os.path.join(OUT_DIR, f\"interm_videos_{video_name}\"), output_path=os.path.join(\n",
    "            OUT_DIR, f\"{video_name}_interpolated{final_extension}\"), fps=fps)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time for {video_name}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     generate_all_interpolations(checkpoint_dir=CHECKPOINT_DIR, input_sub_dir=input_subdir_path, output_dir=OUT_DIR, model_name=MODEL_NAME, resize_specs=resize_specs, fps=fps, duration=duration, extension=interpolation_extension,\n",
    "#                     seed=seed, inference_steps=inference_steps, noise_injection_steps=noise_injection_steps, noise_injection_ratio=noise_injection_ratio, decode_chunk_size=decode_chunk_size, num_frames=num_frames)\n",
    "# except Exception as e:\n",
    "#     print(\"Error while generate_all_interpolations\")\n",
    "#     print(f\"Error: {e}\")\n",
    "#     # i am doing this cuz it is annoying to get out of memory erros every time there is an error. so we will kill the process after it is done/error thrown\n",
    "#     print(\"Killing the kernel processes\")\n",
    "#     env_name = os.path.basename(sys.prefix)\n",
    "#     subprocess.call(\n",
    "#     f\"nvidia-smi | grep {env_name} | awk '{{print $5}}' | xargs -I {{}} kill -9 {{}}\",\n",
    "#     shell=True\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "#     stitch_video_segments(intermediate_videos_dir=os.path.join(OUT_DIR, f\"interm_videos_{input_subdir_name}\"), output_path=os.path.join(\n",
    "#         OUT_DIR, f\"{input_subdir_name}_interpolated{final_extension}\"), fps=fps)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Elapsed time for {video_name}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi | grep $(conda env list | grep '*' | awk '{print $1}') | awk '{print $5}' | xargs -I {} kill -9 {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate the generated video against the original video using several methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the paths and other hyper parameters of the two videos to compare against each other.\n",
    "generated_video = \"office_dude/1.mp4\"\n",
    "# generated_video = \"our_results_strawman/car_drive_interpolated.mp4\"\n",
    "original_video = \"office_dude/2.mp4\"\n",
    "# original_video = \"input/car_drive.mp4\"\n",
    "frame_width = 1024\n",
    "frame_height = 576\n",
    "max_frames = 300\n",
    "\n",
    "# make directories for storing the frames of both of these videos if they dont exist already\n",
    "frames_dir = \"frames\"\n",
    "if os.path.exists(frames_dir):\n",
    "    shutil.rmtree(frames_dir)\n",
    "generated_video_frame_dir = os.path.join(frames_dir, \"input\")\n",
    "original_video_frame_dir = os.path.join(frames_dir, \"output\")\n",
    "if not os.path.exists(frames_dir):\n",
    "    os.makedirs(frames_dir)\n",
    "if not os.path.exists(generated_video_frame_dir):\n",
    "    os.makedirs(generated_video_frame_dir)\n",
    "if not os.path.exists(original_video_frame_dir):\n",
    "    os.makedirs(original_video_frame_dir)\n",
    "\n",
    "original_video_dir = \"/VideoReconstruction/input\"\n",
    "generated_video_dir = \"/VideoReconstruction/our_results_strawman\"\n",
    "\n",
    "# # get file basename form original video dir\n",
    "# for file in os.listdir(original_video_dir):\n",
    "#     if file.endswith(\".mp4\"):\n",
    "#         filename = os.path.splitext(file)[0]\n",
    "#         print(f\"Processing {filename}\")\n",
    "#         generated_video = f\"{generated_video_dir}/{filename}_interpolated.mp4\"\n",
    "#         original_video = f\"{original_video_dir}/{filename}.mp4\"\n",
    "#         # break\n",
    "#         # break both the videos down into frames\n",
    "process_and_extract_frames(input_video_path=generated_video, frames_output_dir=generated_video_frame_dir,\n",
    "                           new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "\n",
    "process_and_extract_frames(input_video_path=original_video, frames_output_dir=original_video_frame_dir,\n",
    "                           new_width=frame_width, new_height=frame_height, max_frames=max_frames)\n",
    "# Pass it the directories with the frames of original and generated video to retrieve the perceptual loss across each frame\n",
    "calculate_lpips_distance(dir0=generated_video_frame_dir,\n",
    "                         dir1=original_video_frame_dir, use_gpu=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-0-27-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
